<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="不使用深度模型框架，只利用Tensor和Autograd实现一个线性回归的训练，要求有输入输出。 模型定义线性回归假设输出与各个输入之间是线性关系： \hat{y} &#x3D; x_{1} \omega _{1} + x_{2} \omega _{2} + x_{3} \omega _{3} + b其中$ \omega $是权重，$ b $是偏差，且均为标量，他们是线性回归模型的参数。模型输出$ \hat">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensor与Autograd实现线性回归训练">
<meta property="og:url" content="http://example.com/2023/07/17/my-first-blog/index.html">
<meta property="og:site_name" content="暑期实习博客">
<meta property="og:description" content="不使用深度模型框架，只利用Tensor和Autograd实现一个线性回归的训练，要求有输入输出。 模型定义线性回归假设输出与各个输入之间是线性关系： \hat{y} &#x3D; x_{1} \omega _{1} + x_{2} \omega _{2} + x_{3} \omega _{3} + b其中$ \omega $是权重，$ b $是偏差，且均为标量，他们是线性回归模型的参数。模型输出$ \hat">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-07-17T12:37:18.595Z">
<meta property="article:modified_time" content="2023-07-20T11:14:01.118Z">
<meta property="article:author" content="时骅">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2023/07/17/my-first-blog/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tensor与Autograd实现线性回归训练 | 暑期实习博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">暑期实习博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/17/my-first-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensor与Autograd实现线性回归训练
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-17 20:37:18" itemprop="dateCreated datePublished" datetime="2023-07-17T20:37:18+08:00">2023-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-20 19:14:01" itemprop="dateModified" datetime="2023-07-20T19:14:01+08:00">2023-07-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>不使用深度模型框架，只利用<strong>Tensor</strong>和<strong>Autograd</strong>实现一个<strong>线性回归</strong>的训练，要求有输入输出。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>线性回归假设输出与各个输入之间是线性关系：</p>
<script type="math/tex; mode=display">\hat{y} = x_{1} \omega _{1} + x_{2} \omega _{2} + x_{3} \omega _{3} + b</script><p>其中$ \omega $是权重，$ b $是偏差，且均为标量，他们是线性回归模型的参数。<br>模型输出$ \hat{y} $是对真实值$ y $（又称标签）的估计。我们通常允许他们之间有一定的误差。<br>而$ x $则称为样本的特征，用来表征样本的特点。</p>
<p>在机器学习里，将衡量误差的函数称为损失函数。这里我们使用平方误差函数，即：</p>
<script type="math/tex; mode=display">l^{(i)}(\omega _{1},\omega _{2},\omega _{3},b)=\frac{1}{2}(\hat{y} ^{(i)}-y^{(i)} ) ^{2}</script><p>其中常数$ \frac{1}{2} $使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降在深度学习中被广泛使用。<br>线性回归模型的过程中，模型的每个参数将作如下迭代：</p>
<script type="math/tex; mode=display">\omega _{1}\gets\omega _{1}-\frac{\eta }{\left |B \right | }\sum_{i\in B}^{} \frac{\partial l^{(i)}(\omega _{1},\omega _{2},\omega _{3},b)}{\partial \omega _{1}}</script><p>在上式中，$ \left |B  \right | $代表每个小批量中的样本个数，$ \eta $ 称作学习率并取正数。</p>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>设训练数据集样本数为1000，输入个数（特征数）为3。给定随机生成的批量样本特征。<br>使用线性回归模型真实权重$\omega =\left [ 2,-2.3,6 \right ] ^{\top } $和偏差$ b=4.2 $以及一个随机噪声项$ \varepsilon $来生成标签。<br>其中噪声项$ \varepsilon $服从均值为0、标准差为0.01的正态分布。<br>生成数据集代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">num_inputs = <span class="number">3</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">2.3</span>, <span class="number">6</span>]</span><br><span class="line">true_b = <span class="number">4.5</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs,</span><br><span class="line">                       dtype=torch.float32)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * features[:, <span class="number">2</span>] + true_b</span><br><span class="line">noise = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32)</span><br><span class="line">labels += noise</span><br></pre></td></tr></table></figure></p>
<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。<br>这里我们定义一个函数：它每次返回batch_size（批量大小）个随机样本的特征和标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历数据集并不断读取小批量数据样本,每次返回batch_size（批量大小）个随机样本的特征和标签。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)]) <span class="comment"># 最后一次可能不足一个batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure></p>
<h3 id="模型函数"><a href="#模型函数" class="headerlink" title="模型函数"></a>模型函数</h3><p>下面是线性回归的矢量计算表达式的实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用mm函数做矩阵乘法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  </span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure><br>用平方损失来定义线性回归的损失函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_pred, y</span>):  </span><br><span class="line">    <span class="keyword">return</span> (y_pred - y.view(y_pred.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><br>以下的sgd函数实现了上一节中介绍的小批量随机梯度下降算法。<br>它通过不断迭代模型参数来优化损失函数。<br>这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。<br>我们将它除以批量大小来得到平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>): </span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br></pre></td></tr></table></figure></p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。<br>之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此打开Autograd。<br>学习率设为0.03，迭代周期个数设为3。<br>批量大小最好为2的幂次，设为8。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#迭代周期</span></span><br><span class="line">batch_size = <span class="number">8</span> <span class="comment">#批量大小</span></span><br></pre></td></tr></table></figure></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>在训练中，我们将多次迭代模型参数。<br>在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），<br>通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。<br>在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):  </span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = squared_loss(linreg(X, w, b), y).<span class="built_in">sum</span>()  <span class="comment"># l是有关小批量X和y的损失之和</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = squared_loss(linreg(features, w, b), labels)</span><br><span class="line">    losses.append(train_l.mean().item())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.012331</span><br><span class="line">epoch 2, loss 0.000054</span><br><span class="line">epoch 3, loss 0.000049</span><br></pre></td></tr></table></figure></p>
<h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p>比较学到的参数和用来生成训练集的真实参数。它们很接近。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[2, -2.3, 6]</span><br><span class="line"> tensor([[ 2.0001],</span><br><span class="line">        [-2.3000],</span><br><span class="line">        [ 5.9997]], requires_grad=True)</span><br><span class="line">4.5</span><br><span class="line"> tensor([4.5000], requires_grad=True)</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2023/07/24/week2/" rel="next" title="伯努利模型">
      伯努利模型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-number">1.</span> <span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.</span> <span class="nav-text">生成数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">3.</span> <span class="nav-text">读取数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">模型函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">参数初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">6.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA"><span class="nav-number">7.</span> <span class="nav-text">模型输出</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">时骅</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">时骅</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
