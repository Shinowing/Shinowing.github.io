<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="利用Transformers代码库中的模型来解决文本分类任务，任务来源于GLUE Benchmark。如下一共九个句子级别的分类任务。自然语言处理（NLP）主要分为自然语言理解（NLU）和自然语言生成（NLG）。为了让NLU任务发挥最大的作用，来自纽约大学、华盛顿大学等机构创建了一个多任务的自然语言理解基准和分析平台，也就是GLUE（General Language Understanding E">
<meta property="og:type" content="article">
<meta property="og:title" content="基于transformers的京东零售NLP任务">
<meta property="og:url" content="http://example.com/2023/08/12/GLUE/index.html">
<meta property="og:site_name" content="暑期实习博客">
<meta property="og:description" content="利用Transformers代码库中的模型来解决文本分类任务，任务来源于GLUE Benchmark。如下一共九个句子级别的分类任务。自然语言处理（NLP）主要分为自然语言理解（NLU）和自然语言生成（NLG）。为了让NLU任务发挥最大的作用，来自纽约大学、华盛顿大学等机构创建了一个多任务的自然语言理解基准和分析平台，也就是GLUE（General Language Understanding E">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/Shinowing/Shinowing.github.io/blob/master/images/GLUE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0%E5%9B%BE.jpg">
<meta property="article:published_time" content="2023-08-12T07:13:36.703Z">
<meta property="article:modified_time" content="2023-08-12T11:33:53.700Z">
<meta property="article:author" content="时骅">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Shinowing/Shinowing.github.io/blob/master/images/GLUE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0%E5%9B%BE.jpg">

<link rel="canonical" href="http://example.com/2023/08/12/GLUE/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>基于transformers的京东零售NLP任务 | 暑期实习博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">暑期实习博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/12/GLUE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于transformers的京东零售NLP任务
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-12 15:13:36 / 修改时间：19:33:53" itemprop="dateCreated datePublished" datetime="2023-08-12T15:13:36+08:00">2023-08-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>利用<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">Transformers</a>代码库中的模型来解决文本分类任务，任务来源于<a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE Benchmark</a>。如下一共九个句子级别的分类任务。<br><img src="https://github.com/Shinowing/Shinowing.github.io/blob/master/images/GLUE%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0%E5%9B%BE.jpg" alt="GLUE数据集描述图"><br>自然语言处理（NLP）主要分为自然语言理解（NLU）和自然语言生成（NLG）。为了让NLU任务发挥最大的作用，来自纽约大学、华盛顿大学等机构创建了一个多任务的自然语言理解基准和分析平台，也就是GLUE（General Language Understanding Evaluation）。<br>GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。<br>下文以CoLA任务为例，展示代码实现过程。</p>
<h3 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h3><p>载入数据需要用到datasets库，该库已经集成好了GLUE数据集，所以本实验直接调用datasets中的load_dataset来加载数据集。这里我们设置一个task_name变量用来储存这次任务的任务名称。raw_datasets是加载完成的原始数据集。数据加载之后会自动缓存，之后再加载相同数据集时会先查看有无缓存，无需重新下载。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GLUE_TASKS = [<span class="string">&quot;cola&quot;</span>, <span class="string">&quot;mnli&quot;</span>,  <span class="string">&quot;mrpc&quot;</span>, <span class="string">&quot;qnli&quot;</span>, <span class="string">&quot;qqp&quot;</span>, <span class="string">&quot;rte&quot;</span>, <span class="string">&quot;sst2&quot;</span>, <span class="string">&quot;stsb&quot;</span>, <span class="string">&quot;wnli&quot;</span>]</span><br><span class="line">task_name = <span class="string">&quot;cola&quot;</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, task_name)</span><br></pre></td></tr></table></figure><br>这个datasets对象本身是一种DatasetDict数据结构. 对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 8551</span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 1043</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="built_in">test</span>: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 1063</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><br>可以看到训练集中有8551个句子，验证集中有1043个，测试集有1063个。每一个都包含sentence、label和idx。<br>给定一个数据切分的key（train、validation或者test）和下标即可查看数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence&#x27;</span>: <span class="string">&quot;Our friends won&#x27;t buy this analysis, let alone the next one we propose.&quot;</span>,</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: 1,</span><br><span class="line"> <span class="string">&#x27;idx&#x27;</span>: 0&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>Transformers是一个非常强大的自然语言处理（NLP）库，提供了许多优秀的NLP处理模型。<br>1.BERT（Bidirectional Encoder Representations from Transformers）：BERT是一种预训练的语言表示模型，通过训练大规模的无标签文本数据来学习通用的语言表示，可以应用于多种NLP任务，如文本分类、命名实体识别、句子关系判断等。<br>2.GPT（Generative Pre-trained Transformer）：GPT是一个基于Transformer的预训练语言模型，主要用于生成文本。它可以生成连贯、有逻辑的句子，被广泛应用于文本生成、对话系统等任务。<br>3.RoBERTa（Robustly Optimized BERT Pretraining Approach）：RoBERTa是对BERT模型的改进版本，通过更大规模的训练数据和更长的训练时间来提高性能。RoBERTa在各种NLP任务中表现出色，特别适用于文本分类、情感分析等任务。<br>4.DistilBERT：DistilBERT是一个经过蒸馏（distillation）的BERT模型，通过保留BERT的核心结构和知识来减小模型的大小和计算成本，适用于资源受限的环境。<br>5.XLNet：XLNet是一种自回归的预训练语言模型，采用了排列语言模型（permutation language modeling）的方法，可以更好地处理长文本和远距离依赖关系。</p>
<p>鉴于手头工作设备只有一台办公笔记本，算力资源有限，所以本实验选取了distilbert-base-uncased模型，这是基于DistilBERT模型的基本版本，相比于BERT架构的bert-base-uncased模型，它更为精简与轻量化，可以在保持相对较高性能的同时，具有更小的规模和更快的训练速度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br></pre></td></tr></table></figure><br>以下代码用于计算标签数量。通过task_name的值判断任务类型，如果是回归任务（stsb），则num_labels为1；如果不是回归任务，则根据训练集的标签列表计算标签的数量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">is_regression = task_name == <span class="string">&quot;stsb&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> is_regression:</span><br><span class="line">    label_list = raw_datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">&quot;label&quot;</span>].names</span><br><span class="line">    num_labels = <span class="built_in">len</span>(label_list)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_labels = <span class="number">1</span></span><br></pre></td></tr></table></figure><br>然后使用AutoModelForSequenceClassification这个类，通过from_pretrained方法下载并加载模型，同时也会对模型进行缓存。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure></p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>在使用神经网络处理自然语言处理任务时，我们需要在将数据喂入模型之前对数据进行预处理，将数据从字符串转换为神经网络可以接受的格式，一般会分为如下几步：<br>（1）分词：使用分词器对文本数据进行分词（字、字词）；<br>（2）构建词典：根据数据集分词的结果，构建词典映射（这一步并不绝对，如果采用预训练词向量，词典映射要根据词向量文件进行处理）；<br>（3）数据转换：根据构建好的词典，将分词处理后的数据做映射，将文本序列转换为数字序列；<br>（4）数据填充与截断：在以batch输入到模型的方式中，需要对过短的数据进行填充，过长的数据进行截断，保证数据长度符合模型能接受的范围，同时batch内的数据维度大小一致。<br>预处理的工具叫Tokenizer，它可以快速的实现上述全部工作，其功能就是将文本转换为神经网络可以处理的数据。Tokenizer首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。<br>使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这里use_fast=True要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>task_to_keys表示不同数据和对应的数据格式。<br>preprocess_function接受一个examples参数，examples是一个包含输入数据的字典。在函数内部，根据sentence1_key和sentence2_key的值，将相应的句子作为参数传递给tokenizer进行分词。如果sentence2_key为None，则只对sentence1_key进行分词；否则，同时对sentence1_key和sentence2_key进行分词。最后，使用raw_datasets的map方法，将preprocess_function应用于数据集中的每个示例，并通过batched=True指定以批量方式进行处理。<br>tokenized_datasets就是经过分词处理后的数据集。<br>接下来，创建了一个DataCollatorWithPadding对象，该对象使用tokenizer进行数据的填充操作，以便将不同长度的序列批量化。<br>这样，经过数据预处理和填充后的数据集可以用于模型的训练或评估。<br>preprocess_function函数可以处理单个样本，也可以对多个样本进行处理。如果输入是多个样本，那么返回的是一个list。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">task_to_keys = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>: (<span class="string">&quot;question&quot;</span>, <span class="string">&quot;sentence&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>: (<span class="string">&quot;question1&quot;</span>, <span class="string">&quot;question2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">&#125;</span><br><span class="line">sentence1_key, sentence2_key = task_to_keys[task_name]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">        <span class="comment"># Tokenize the texts</span></span><br><span class="line">    args = (</span><br><span class="line">         (examples[sentence1_key],) <span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (examples[sentence1_key], examples[sentence2_key])</span><br><span class="line">     )</span><br><span class="line">    result = tokenizer(*args, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure><br>在之前的尝试中，预处理部分到这里就结束了，之后通过微调模型，修改trainer参数之后，用trainer.Train()函数即可训练模型，然而该运行环境只能在CPU上运行，这导致训练速度过慢。为了可以利用CUDA运行，之后采用了Pytorch进行数据训练，所以还需要进一步的预处理工作。<br>下面这段代码对经过分词处理后的数据集进行了一些操作和修改。<br>首先，通过判断sentence2_key是否存在于列名中，来确定是否需要移除sentence2_key列。如果sentence2_key存在，那么使用remove_columns方法将其从数据集中移除。接下来移除了sentence1_key和”idx”列。这样移除模型不需要的数据是为了简化数据集结构，提高训练和评估的效率，并确保数据与模型之间的兼容性。<br>然后，使用rename_column方法将原来名为”label”的列重命名为”labels”。最后，使用set_format(“torch”)方法将数据集的格式设置为PyTorch格式。<br>最终，通过tokenized_datasets[“train”].column_names可以获取到修改后的训练集数据的列名。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> sentence2_key <span class="keyword">in</span> tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names:</span><br><span class="line">    tokenized_datasets = tokenized_datasets.remove_columns([sentence2_key])</span><br><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([sentence1_key,<span class="string">&quot;idx&quot;</span>])</span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)</span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>]</span><br></pre></td></tr></table></figure><br>接下来这段代码定义了我们的数据加载器。我们使用DataLoader类从tokenized_datasets中加载训练集和验证集的数据。<br>对于训练集，我们使用了shuffle=True来打乱数据顺序，batch_size=8表示每个batch的样本数量为8，collate_fn=data_collator表示在组装batch时使用动态填充。对于验证集也是同样。我们还根据任务名称选择了相应的验证集键，如果任务名称是”mnli”，则使用”validation_matched”键，否则使用”validation”键。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">validation_key = <span class="string">&quot;validation_matched&quot;</span> <span class="keyword">if</span> task_name == <span class="string">&quot;mnli&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation&quot;</span></span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>], shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[validation_key], batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>为了快速检查数据处理中没有错误，我们可以这样检查一个批次：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;labels&#x27;</span>: torch.Size([8]),</span><br><span class="line"> <span class="string">&#x27;input_ids&#x27;</span>: torch.Size([8, 15]),</span><br><span class="line"> <span class="string">&#x27;attention_mask&#x27;</span>: torch.Size([8, 15])&#125;</span><br></pre></td></tr></table></figure><br>为了确保训练过程中一切顺利，我们将批次传递给模型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(**batch)</span><br><span class="line"><span class="built_in">print</span>(outputs.loss, outputs.logits.shape)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.6666, grad_fn=&lt;NllLossBackward0&gt;) torch.Size([8, 2])</span><br></pre></td></tr></table></figure></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>首先定义一些超参数，为不同任务的数据集确定不同的迭代周期与学习率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">epochs=&#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>:<span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>:<span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line">learning_rate=&#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>:<span class="number">5e-5</span>,</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>:<span class="number">5e-5</span>,</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>:<span class="number">3e-5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>从transformers库导入AdamW、get_scheduler优化器，根据任务名称设置了学习率，并使用AdamW优化器对模型的参数进行优化。接着，计算了总的训练步数，即num_training_steps，它等于训练集的样本数乘以训练的轮数。最后，使用get_scheduler函数创建了一个线性学习率调度器，并将其与优化器关联起来。其中，num_warmup_steps设置为0，表示不进行学习率的预热。<br>为了进一步提高训练速度，我还使用了accelerate库。首先创建了一个Accelerator对象，然后使用accelerator.prepare()函数对训练集和验证集的数据加载器、模型和优化器进行了一些准备工作。这个函数会自动将模型和数据加载器移到合适的设备上，并使用混合精度训练（如果可用）以加速训练过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, get_scheduler</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=learning_rate[task_name])</span><br><span class="line">train_dl, eval_dl, model, optimizer = accelerator.prepare(</span><br><span class="line">    train_dataloader, eval_dataloader, model, optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">num_epochs = epochs[task_name]</span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dl)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(num_training_steps)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3207</span><br></pre></td></tr></table></figure><br>然后将设备移至GPU。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">device</span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure><br>接下来定义一个模型训练函数，引入tqdm库制作了一个进度条使训练过程可视化。然后，将模型设置为训练模式，即model.train()。接下来，使用两个嵌套的循环进行训练。外层循环是根据num_epochs的值来控制训练的轮数。内层循环是遍历训练数据加载器train_dl中的每个批次。对于每个批次，我们将其输入到模型中，得到模型的输出outputs。然后，我们从outputs中获取损失值loss。接着，我们使用accelerator.backward()函数对损失值进行反向传播，并执行优化器的step()函数来更新模型的参数。然后，我们使用学习率调度器的step()函数来更新学习率。最后，我们使用优化器的zero_grad()函数来清零梯度，以便进行下一个batch的训练。在每个批次的训练完成后，我们使用进度条的update()函数更新进度条的状态。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_function</span>():</span><br><span class="line">    progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_dl:</span><br><span class="line">            outputs = model(**batch)</span><br><span class="line">            loss = outputs.loss</span><br><span class="line">            accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>查看最多可用于训练的GPU数量，如果有多个GPU，本实验可以采用分布式训练，大幅提高训练速度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_gpus = torch.cuda.device_count()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of available GPUs:&quot;</span>, num_gpus)</span><br></pre></td></tr></table></figure><br>设备只有一个GPU。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of available GPUs: 1</span><br></pre></td></tr></table></figure><br>在notebook上运行，开始训练。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> notebook_launcher</span><br><span class="line"></span><br><span class="line">notebook_launcher(training_function,num_processes=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>接下来对训练结果进行评估。导入了evaluate模块和numpy库，并定义了一个空的pred数组。<br>然后，使用evaluate.load()函数加载了相应任务的评估方式。每个文本分类任务的评估方式不同，具体如下显示：<br>for CoLA: Matthews Correlation Coefficient<br>for MNLI (matched or mismatched): Accuracy<br>for MRPC: Accuracy and F1 score<br>for QNLI: Accuracy<br>for QQP: Accuracy and F1 score<br>for RTE: Accuracy<br>for SST-2: Accuracy<br>for STS-B: Pearson Correlation Coefficient and Spearman’s_Rank_Correlation_Coefficient<br>for WNLI: Accuracy<br>接下来，将模型设置为评估模式，即model.eval()。然后，使用一个循环遍历验证数据加载器eval_dataloader中的每个batch。对于每个batch，我们将其转移到设备上，并使用torch.no_grad()上下文管理器禁用梯度计算。在禁用梯度计算的情况下，我们将batch输入到模型中，得到模型的输出outputs。然后，我们从outputs中获取logits，并使用torch.argmax()函数找到每个样本的预测类别。接着，我们使用评估器的add_batch()函数将预测值和真实标签添加到评估器中进行计算指标。使用np.concatenate()函数将它们添加到pred数组中。最后，我们使用评估器的compute()函数计算并打印出评估指标的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pred = np.array([], dtype=np.int32)</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, task_name)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">    pred = np.concatenate((pred, predictions.cpu().numpy()), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">metric.compute() </span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;matthews_correlation&#x27;</span>: 0.549825623859248&#125;</span><br></pre></td></tr></table></figure><br>预测结果保存成csv文件形式，表头包含序号和预测标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;qnli.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;IDs,labels\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, y))</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/07/food/" rel="prev" title="图片分类">
      <i class="fa fa-chevron-left"></i> 图片分类
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-number">1.</span> <span class="nav-text">载入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">2.</span> <span class="nav-text">模型选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">4.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0"><span class="nav-number">5.</span> <span class="nav-text">评估</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">时骅</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">时骅</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
