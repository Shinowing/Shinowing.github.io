<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="暑期实习博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="暑期实习博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="时骅">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>暑期实习博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">暑期实习博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/12/GLUE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/12/GLUE/" class="post-title-link" itemprop="url">基于transformers的京东零售NLP任务</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-12 15:13:36 / 修改时间：19:57:27" itemprop="dateCreated datePublished" datetime="2023-08-12T15:13:36+08:00">2023-08-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>利用<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">Transformers</a>代码库中的模型来解决文本分类任务，任务来源于<a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE Benchmark</a>。如下一共九个句子级别的分类任务。<br><img src="/2023/08/12/GLUE/GLUE数据集描述图.jpg" alt="GLUE数据集描述图"><br>自然语言处理（NLP）主要分为自然语言理解（NLU）和自然语言生成（NLG）。为了让NLU任务发挥最大的作用，来自纽约大学、华盛顿大学等机构创建了一个多任务的自然语言理解基准和分析平台，也就是GLUE（General Language Understanding Evaluation）。<br>GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像BERT、XLNet、RoBERTa、ERINE、T5等知名模型都会在此基准上进行测试。<br>下文以CoLA任务为例，展示代码实现过程。</p>
<h3 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h3><p>载入数据需要用到datasets库，该库已经集成好了GLUE数据集，所以本实验直接调用datasets中的load_dataset来加载数据集。这里我们设置一个task_name变量用来储存这次任务的任务名称。raw_datasets是加载完成的原始数据集。数据加载之后会自动缓存，之后再加载相同数据集时会先查看有无缓存，无需重新下载。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GLUE_TASKS = [<span class="string">&quot;cola&quot;</span>, <span class="string">&quot;mnli&quot;</span>,  <span class="string">&quot;mrpc&quot;</span>, <span class="string">&quot;qnli&quot;</span>, <span class="string">&quot;qqp&quot;</span>, <span class="string">&quot;rte&quot;</span>, <span class="string">&quot;sst2&quot;</span>, <span class="string">&quot;stsb&quot;</span>, <span class="string">&quot;wnli&quot;</span>]</span><br><span class="line">task_name = <span class="string">&quot;cola&quot;</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, task_name)</span><br></pre></td></tr></table></figure><br>这个datasets对象本身是一种DatasetDict数据结构. 对于训练集、验证集和测试集，只需要使用对应的key（train，validation，test）即可得到相应的数据。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 8551</span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 1043</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="built_in">test</span>: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: 1063</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><br>可以看到训练集中有8551个句子，验证集中有1043个，测试集有1063个。每一个都包含sentence、label和idx。<br>给定一个数据切分的key（train、validation或者test）和下标即可查看数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sentence&#x27;</span>: <span class="string">&quot;Our friends won&#x27;t buy this analysis, let alone the next one we propose.&quot;</span>,</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: 1,</span><br><span class="line"> <span class="string">&#x27;idx&#x27;</span>: 0&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>Transformers是一个非常强大的自然语言处理（NLP）库，提供了许多优秀的NLP处理模型。<br>1.BERT（Bidirectional Encoder Representations from Transformers）：BERT是一种预训练的语言表示模型，通过训练大规模的无标签文本数据来学习通用的语言表示，可以应用于多种NLP任务，如文本分类、命名实体识别、句子关系判断等。<br>2.GPT（Generative Pre-trained Transformer）：GPT是一个基于Transformer的预训练语言模型，主要用于生成文本。它可以生成连贯、有逻辑的句子，被广泛应用于文本生成、对话系统等任务。<br>3.RoBERTa（Robustly Optimized BERT Pretraining Approach）：RoBERTa是对BERT模型的改进版本，通过更大规模的训练数据和更长的训练时间来提高性能。RoBERTa在各种NLP任务中表现出色，特别适用于文本分类、情感分析等任务。<br>4.DistilBERT：DistilBERT是一个经过蒸馏（distillation）的BERT模型，通过保留BERT的核心结构和知识来减小模型的大小和计算成本，适用于资源受限的环境。<br>5.XLNet：XLNet是一种自回归的预训练语言模型，采用了排列语言模型（permutation language modeling）的方法，可以更好地处理长文本和远距离依赖关系。</p>
<p>鉴于手头工作设备只有一台办公笔记本，算力资源有限，所以本实验选取了distilbert-base-uncased模型，这是基于DistilBERT模型的基本版本，相比于BERT架构的bert-base-uncased模型，它更为精简与轻量化，可以在保持相对较高性能的同时，具有更小的规模和更快的训练速度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br></pre></td></tr></table></figure><br>以下代码用于计算标签数量。通过task_name的值判断任务类型，如果是回归任务（stsb），则num_labels为1；如果不是回归任务，则根据训练集的标签列表计算标签的数量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">is_regression = task_name == <span class="string">&quot;stsb&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> is_regression:</span><br><span class="line">    label_list = raw_datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">&quot;label&quot;</span>].names</span><br><span class="line">    num_labels = <span class="built_in">len</span>(label_list)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_labels = <span class="number">1</span></span><br></pre></td></tr></table></figure><br>然后使用AutoModelForSequenceClassification这个类，通过from_pretrained方法下载并加载模型，同时也会对模型进行缓存。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure></p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>在使用神经网络处理自然语言处理任务时，我们需要在将数据喂入模型之前对数据进行预处理，将数据从字符串转换为神经网络可以接受的格式，一般会分为如下几步：<br>（1）分词：使用分词器对文本数据进行分词（字、字词）；<br>（2）构建词典：根据数据集分词的结果，构建词典映射（这一步并不绝对，如果采用预训练词向量，词典映射要根据词向量文件进行处理）；<br>（3）数据转换：根据构建好的词典，将分词处理后的数据做映射，将文本序列转换为数字序列；<br>（4）数据填充与截断：在以batch输入到模型的方式中，需要对过短的数据进行填充，过长的数据进行截断，保证数据长度符合模型能接受的范围，同时batch内的数据维度大小一致。<br>预处理的工具叫Tokenizer，它可以快速的实现上述全部工作，其功能就是将文本转换为神经网络可以处理的数据。Tokenizer首先对输入进行tokenize，然后将tokens转化为预模型中需要对应的token ID，再转化为模型需要的输入格式。<br>使用AutoTokenizer.from_pretrained方法实例化我们的tokenizer，这里use_fast=True要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint,use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>task_to_keys表示不同数据和对应的数据格式。<br>preprocess_function接受一个examples参数，examples是一个包含输入数据的字典。在函数内部，根据sentence1_key和sentence2_key的值，将相应的句子作为参数传递给tokenizer进行分词。如果sentence2_key为None，则只对sentence1_key进行分词；否则，同时对sentence1_key和sentence2_key进行分词。最后，使用raw_datasets的map方法，将preprocess_function应用于数据集中的每个示例，并通过batched=True指定以批量方式进行处理。<br>tokenized_datasets就是经过分词处理后的数据集。<br>接下来，创建了一个DataCollatorWithPadding对象，该对象使用tokenizer进行数据的填充操作，以便将不同长度的序列批量化。<br>这样，经过数据预处理和填充后的数据集可以用于模型的训练或评估。<br>preprocess_function函数可以处理单个样本，也可以对多个样本进行处理。如果输入是多个样本，那么返回的是一个list。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">task_to_keys = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>: (<span class="string">&quot;question&quot;</span>, <span class="string">&quot;sentence&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>: (<span class="string">&quot;question1&quot;</span>, <span class="string">&quot;question2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">&#125;</span><br><span class="line">sentence1_key, sentence2_key = task_to_keys[task_name]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_function</span>(<span class="params">examples</span>):</span><br><span class="line">        <span class="comment"># Tokenize the texts</span></span><br><span class="line">    args = (</span><br><span class="line">         (examples[sentence1_key],) <span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> (examples[sentence1_key], examples[sentence2_key])</span><br><span class="line">     )</span><br><span class="line">    result = tokenizer(*args, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure><br>在之前的尝试中，预处理部分到这里就结束了，之后通过微调模型，修改trainer参数之后，用trainer.Train()函数即可训练模型，然而该运行环境只能在CPU上运行，这导致训练速度过慢。为了可以利用CUDA运行，之后采用了Pytorch进行数据训练，所以还需要进一步的预处理工作。<br>下面这段代码对经过分词处理后的数据集进行了一些操作和修改。<br>首先，通过判断sentence2_key是否存在于列名中，来确定是否需要移除sentence2_key列。如果sentence2_key存在，那么使用remove_columns方法将其从数据集中移除。接下来移除了sentence1_key和”idx”列。这样移除模型不需要的数据是为了简化数据集结构，提高训练和评估的效率，并确保数据与模型之间的兼容性。<br>然后，使用rename_column方法将原来名为”label”的列重命名为”labels”。最后，使用set_format(“torch”)方法将数据集的格式设置为PyTorch格式。<br>最终，通过tokenized_datasets[“train”].column_names可以获取到修改后的训练集数据的列名。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> sentence2_key <span class="keyword">in</span> tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names:</span><br><span class="line">    tokenized_datasets = tokenized_datasets.remove_columns([sentence2_key])</span><br><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([sentence1_key,<span class="string">&quot;idx&quot;</span>])</span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)</span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>]</span><br></pre></td></tr></table></figure><br>接下来这段代码定义了我们的数据加载器。我们使用DataLoader类从tokenized_datasets中加载训练集和验证集的数据。<br>对于训练集，我们使用了shuffle=True来打乱数据顺序，batch_size=8表示每个batch的样本数量为8，collate_fn=data_collator表示在组装batch时使用动态填充。对于验证集也是同样。我们还根据任务名称选择了相应的验证集键，如果任务名称是”mnli”，则使用”validation_matched”键，否则使用”validation”键。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">validation_key = <span class="string">&quot;validation_matched&quot;</span> <span class="keyword">if</span> task_name == <span class="string">&quot;mnli&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation&quot;</span></span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>], shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[validation_key], batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br>为了快速检查数据处理中没有错误，我们可以这样检查一个批次：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;labels&#x27;</span>: torch.Size([8]),</span><br><span class="line"> <span class="string">&#x27;input_ids&#x27;</span>: torch.Size([8, 15]),</span><br><span class="line"> <span class="string">&#x27;attention_mask&#x27;</span>: torch.Size([8, 15])&#125;</span><br></pre></td></tr></table></figure><br>为了确保训练过程中一切顺利，我们将批次传递给模型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(**batch)</span><br><span class="line"><span class="built_in">print</span>(outputs.loss, outputs.logits.shape)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.6666, grad_fn=&lt;NllLossBackward0&gt;) torch.Size([8, 2])</span><br></pre></td></tr></table></figure></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>首先定义一些超参数，为不同任务的数据集确定不同的迭代周期与学习率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">epochs=&#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>:<span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>:<span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line">learning_rate=&#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>:<span class="number">5e-5</span>,</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>:<span class="number">3e-5</span>,</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>:<span class="number">5e-5</span>,</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>:<span class="number">2e-5</span>,</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>:<span class="number">3e-5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>从transformers库导入AdamW、get_scheduler优化器，根据任务名称设置了学习率，并使用AdamW优化器对模型的参数进行优化。接着，计算了总的训练步数，即num_training_steps，它等于训练集的样本数乘以训练的轮数。最后，使用get_scheduler函数创建了一个线性学习率调度器，并将其与优化器关联起来。其中，num_warmup_steps设置为0，表示不进行学习率的预热。<br>为了进一步提高训练速度，我还使用了accelerate库。首先创建了一个Accelerator对象，然后使用accelerator.prepare()函数对训练集和验证集的数据加载器、模型和优化器进行了一些准备工作。这个函数会自动将模型和数据加载器移到合适的设备上，并使用混合精度训练（如果可用）以加速训练过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW, get_scheduler</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=learning_rate[task_name])</span><br><span class="line">train_dl, eval_dl, model, optimizer = accelerator.prepare(</span><br><span class="line">    train_dataloader, eval_dataloader, model, optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">num_epochs = epochs[task_name]</span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dl)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(num_training_steps)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3207</span><br></pre></td></tr></table></figure><br>然后将设备移至GPU。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">device</span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure><br>接下来定义一个模型训练函数，引入tqdm库制作了一个进度条使训练过程可视化。然后，将模型设置为训练模式，即model.train()。接下来，使用两个嵌套的循环进行训练。外层循环是根据num_epochs的值来控制训练的轮数。内层循环是遍历训练数据加载器train_dl中的每个批次。对于每个批次，我们将其输入到模型中，得到模型的输出outputs。然后，我们从outputs中获取损失值loss。接着，我们使用accelerator.backward()函数对损失值进行反向传播，并执行优化器的step()函数来更新模型的参数。然后，我们使用学习率调度器的step()函数来更新学习率。最后，我们使用优化器的zero_grad()函数来清零梯度，以便进行下一个batch的训练。在每个批次的训练完成后，我们使用进度条的update()函数更新进度条的状态。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_function</span>():</span><br><span class="line">    progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_dl:</span><br><span class="line">            outputs = model(**batch)</span><br><span class="line">            loss = outputs.loss</span><br><span class="line">            accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>查看最多可用于训练的GPU数量，如果有多个GPU，本实验可以采用分布式训练，大幅提高训练速度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_gpus = torch.cuda.device_count()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of available GPUs:&quot;</span>, num_gpus)</span><br></pre></td></tr></table></figure><br>设备只有一个GPU。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Number of available GPUs: 1</span><br></pre></td></tr></table></figure><br>在notebook上运行，开始训练。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> notebook_launcher</span><br><span class="line"></span><br><span class="line">notebook_launcher(training_function,num_processes=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>接下来对训练结果进行评估。导入了evaluate模块和numpy库，并定义了一个空的pred数组。<br>然后，使用evaluate.load()函数加载了相应任务的评估方式。每个文本分类任务的评估方式不同，具体如下显示：<br>for CoLA: Matthews Correlation Coefficient<br>for MNLI (matched or mismatched): Accuracy<br>for MRPC: Accuracy and F1 score<br>for QNLI: Accuracy<br>for QQP: Accuracy and F1 score<br>for RTE: Accuracy<br>for SST-2: Accuracy<br>for STS-B: Pearson Correlation Coefficient and Spearman’s_Rank_Correlation_Coefficient<br>for WNLI: Accuracy<br>接下来，将模型设置为评估模式，即model.eval()。然后，使用一个循环遍历验证数据加载器eval_dataloader中的每个batch。对于每个batch，我们将其转移到设备上，并使用torch.no_grad()上下文管理器禁用梯度计算。在禁用梯度计算的情况下，我们将batch输入到模型中，得到模型的输出outputs。然后，我们从outputs中获取logits，并使用torch.argmax()函数找到每个样本的预测类别。接着，我们使用评估器的add_batch()函数将预测值和真实标签添加到评估器中进行计算指标。使用np.concatenate()函数将它们添加到pred数组中。最后，我们使用评估器的compute()函数计算并打印出评估指标的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pred = np.array([], dtype=np.int32)</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, task_name)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">    pred = np.concatenate((pred, predictions.cpu().numpy()), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">metric.compute() </span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;matthews_correlation&#x27;</span>: 0.549825623859248&#125;</span><br></pre></td></tr></table></figure><br>预测结果保存成csv文件形式，表头包含序号和预测标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;qnli.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;IDs,labels\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, y))</span><br></pre></td></tr></table></figure><br><img src="/2023/08/12/GLUE/CoLA.png" alt="结果图"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/07/food/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/07/food/" class="post-title-link" itemprop="url">图片分类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-07 19:20:45" itemprop="dateCreated datePublished" datetime="2023-08-07T19:20:45+08:00">2023-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-10 12:56:41" itemprop="dateModified" datetime="2023-08-10T12:56:41+08:00">2023-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>数据文件夹中均是食物的照片，共有11类，Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.我们要创建一个CNN，用来实现食物的分类。<br>数据集有训练集、验证集、测试集。训练集和验证集带标签，测试集不带标签。</p>
<h3 id="库导入"><a href="#库导入" class="headerlink" title="库导入"></a>库导入</h3><p>首先导入各种库，并设置了一个随机种子，这将确保实验结果可以重现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">_exp_name = <span class="string">&quot;sample&quot;</span></span><br><span class="line"><span class="comment"># Import necessary packages.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># &quot;ConcatDataset&quot; and &quot;Subset&quot; are possibly useful when doing semi-supervised learning.</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset, DataLoader, Subset, Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> DatasetFolder, VisionDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for the progress bar.</span></span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">myseed = <span class="number">6666</span>  <span class="comment"># set a random seed for reproducibility</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"><span class="comment">#torch.backends.cudnn.enable = True</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">np.random.seed(myseed)</span><br><span class="line">random.seed(myseed)</span><br><span class="line">torch.manual_seed(myseed)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed_all(myseed)</span><br></pre></td></tr></table></figure></p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在测试和验证阶段通常不需要数据增强，只需要将PIL图像调整大小并转换为张量。可以使用transforms.Resize将图像大小调整为128x128，<br>并使用transforms.ToTensor将其转换为张量。这样可以确保测试和验证阶段的一致性，使结果可比较。<br>然而，如果希望在测试阶段使用数据增强，您可以使用train_tfm来产生图像，然后使用集成方法进行测试。train_tfm包含了一系列的数据增强操作，如随机裁剪、水平翻转、垂直翻转、随机旋转、随机仿射变换和随机灰度转换。这些增强操作可以增加模型的鲁棒性，提高模型在测试集上的性能。<br>数据增强在训练阶段通常是必需的，因为它可以增加训练数据的多样性，提高模型的泛化能力。但在测试和验证阶段，我们通常希望评估模型在真实数据上的性能，因此不需要进行数据增强。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normally, We don&#x27;t need augmentations in testing and validation.</span></span><br><span class="line"><span class="comment"># All we need here is to resize the PIL image and transform it into Tensor.</span></span><br><span class="line">test_tfm = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment">#transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># However, it is also possible to use augmentation in the testing phase.</span></span><br><span class="line"><span class="comment"># You may use train_tfm to produce a variety of images and then test using ensemble methods</span></span><br><span class="line">train_tfm = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop((<span class="number">128</span>, <span class="number">128</span>), scale=(<span class="number">0.7</span>, <span class="number">1.0</span>)),</span><br><span class="line">    transforms.RandomHorizontalFlip(<span class="number">0.5</span>),</span><br><span class="line">    transforms.RandomVerticalFlip(<span class="number">0.5</span>),</span><br><span class="line">    transforms.RandomRotation(<span class="number">180</span>),</span><br><span class="line">    transforms.RandomAffine(<span class="number">30</span>),</span><br><span class="line">    transforms.RandomGrayscale(p=<span class="number">0.2</span>),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br></pre></td></tr></table></figure></p>
<h3 id="数据集定义"><a href="#数据集定义" class="headerlink" title="数据集定义"></a>数据集定义</h3><p>创建数据集类FoodDataset，用于加载图像数据集并进行相应的数据预处理。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FoodDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,path=<span class="literal">None</span>,tfm=test_tfm,files=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FoodDataset).__init__()</span><br><span class="line">        self.path = path</span><br><span class="line">        <span class="keyword">if</span> path:</span><br><span class="line">            self.files = <span class="built_in">sorted</span>([os.path.join(path, x) <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(path) <span class="keyword">if</span> x.endswith(<span class="string">&quot;.jpg&quot;</span>)])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.files = files</span><br><span class="line">        self.transform = tfm</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Num of element: &#x27;</span>, <span class="built_in">len</span>(self.files))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.files)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        fname = self.files[idx]</span><br><span class="line">        im = Image.<span class="built_in">open</span>(fname)</span><br><span class="line">        im = self.transform(im)</span><br><span class="line">        <span class="comment">#im = self.data[idx]</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            label = <span class="built_in">int</span>(fname.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            label = -<span class="number">1</span> <span class="comment"># test has no label</span></span><br><span class="line">        <span class="keyword">return</span> im,label</span><br></pre></td></tr></table></figure><br>接下来是对残差块（Residual Block）的定义，它是深度残差网络（ResNet）的基本组成单元。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Residual_Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ic, oc, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">        <span class="comment"># torch.nn.MaxPool2d(kernel_size, stride, padding)</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(ic, oc, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(oc),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(oc, oc, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(oc),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">        self.downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> (ic != oc):</span><br><span class="line">            self.downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ic, oc, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">                nn.BatchNorm2d(oc),</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.downsample:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line">            </span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> self.relu(out)</span><br></pre></td></tr></table></figure><br>接下来是对分类器模型的定义，它基于残差块构建了一个深度卷积神经网络。<br>首先，定义了一个预处理卷积层preconv，它包含一个7x7的卷积操作、批归一化和ReLU激活函数。<br>接下来，通过调用make_residual方法构建了四个残差层，分别是layer0、layer1、layer2和layer3。每个残差层都使用make_residual方法生成。<br>最后，定义了一个全连接层fc，它包含了一系列的操作，包括Dropout、线性变换、批归一化和ReLU激活函数。最后一层线性变换的输出维度为num_classes，表示最终的分类结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, num_layers, num_classes=<span class="number">11</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.preconv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.layer0 = self.make_residual(block, <span class="number">32</span>, <span class="number">64</span>,  num_layers[<span class="number">0</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer1 = self.make_residual(block, <span class="number">64</span>, <span class="number">128</span>, num_layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer2 = self.make_residual(block, <span class="number">128</span>, <span class="number">256</span>, num_layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_residual(block, <span class="number">256</span>, <span class="number">512</span>, num_layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#self.avgpool = nn.AvgPool2d(2)</span></span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Sequential(            </span><br><span class="line">            nn.Dropout(<span class="number">0.4</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">11</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_residual</span>(<span class="params">self, block, ic, oc, num_layer, stride=<span class="number">1</span></span>):</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(ic, oc, stride))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_layer):</span><br><span class="line">            layers.append(block(oc, oc))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># [3, 128, 128]</span></span><br><span class="line">        out = self.preconv(x)  <span class="comment"># [32, 64, 64]</span></span><br><span class="line">        out = self.layer0(out) <span class="comment"># [64, 32, 32]</span></span><br><span class="line">        out = self.layer1(out) <span class="comment"># [128, 16, 16]</span></span><br><span class="line">        out = self.layer2(out) <span class="comment"># [256, 8, 8]</span></span><br><span class="line">        out = self.layer3(out) <span class="comment"># [512, 4, 4]</span></span><br><span class="line">        <span class="comment">#out = self.avgpool(out) # [512, 2, 2]</span></span><br><span class="line">        out = self.fc(out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)) </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><br>最后定义两个损失函数。<br>FocalLoss是一种改进的交叉熵损失函数，用于解决类别不平衡问题。<br>MyCrossEntropy是一个简单的交叉熵损失函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, class_num, alpha=<span class="literal">None</span>, gamma=<span class="number">2</span>, size_average=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.alpha = Variable(torch.ones(class_num, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(alpha, Variable):</span><br><span class="line">                self.alpha = alpha</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.alpha = Variable(alpha)</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.class_num = class_num</span><br><span class="line">        self.size_average = size_average</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, targets</span>):</span><br><span class="line">        N = inputs.size(<span class="number">0</span>)</span><br><span class="line">        C = inputs.size(<span class="number">1</span>)</span><br><span class="line">        P = F.softmax(inputs, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        class_mask = inputs.data.new(N, C).fill_(<span class="number">0</span>)</span><br><span class="line">        class_mask = Variable(class_mask)</span><br><span class="line">        ids = targets.view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        class_mask.scatter_(<span class="number">1</span>, ids.data, <span class="number">1.</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> inputs.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.alpha.is_cuda:</span><br><span class="line">            self.alpha = self.alpha.cuda()</span><br><span class="line">        alpha = self.alpha[ids.data.view(-<span class="number">1</span>)]</span><br><span class="line">        probs = (P*class_mask).<span class="built_in">sum</span>(<span class="number">1</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        log_p = probs.log()</span><br><span class="line">        </span><br><span class="line">        batch_loss = -alpha*(torch.<span class="built_in">pow</span>((<span class="number">1</span>-probs), self.gamma))*log_p</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.size_average:</span><br><span class="line">            loss = batch_loss.mean()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = batch_loss.<span class="built_in">sum</span>()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyCrossEntropy</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, class_num</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><br>定义一些参数，并准备数据集。将训练集目录与验证集目录进行路径拼接得到完整的文件路径列表。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">num_layers = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>] <span class="comment"># residual number layers</span></span><br><span class="line">alpha = torch.Tensor([<span class="number">1</span>, <span class="number">2.3</span>, <span class="number">0.66</span>, <span class="number">1</span>, <span class="number">1.1</span>, <span class="number">0.75</span>, <span class="number">2.3</span>, <span class="number">3.5</span>, <span class="number">1.1</span>, <span class="number">0.66</span>, <span class="number">1.4</span>])</span><br><span class="line">n_epochs = <span class="number">300</span></span><br><span class="line">patience = <span class="number">16</span> <span class="comment"># If no improvement in &#x27;patience&#x27; epochs, early stop</span></span><br><span class="line">k_fold = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_dir = <span class="string">&quot;food11/training&quot;</span></span><br><span class="line">val_dir = <span class="string">&quot;food11/validation&quot;</span></span><br><span class="line"></span><br><span class="line">train_files = [os.path.join(train_dir, x) <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(train_dir) <span class="keyword">if</span> x.endswith(<span class="string">&#x27;.jpg&#x27;</span>)]</span><br><span class="line">val_files = [os.path.join(val_dir, x) <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(val_dir) <span class="keyword">if</span> x.endswith(<span class="string">&#x27;.jpg&#x27;</span>)]</span><br><span class="line">total_files = train_files + val_files</span><br><span class="line">random.shuffle(total_files)</span><br><span class="line"></span><br><span class="line">num = <span class="built_in">len</span>(total_files) // k_fold</span><br></pre></td></tr></table></figure></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>下面是一个训练和验证的循环，其中包含了模型的初始化、数据集的准备、训练和验证的过程。<br>首先，代码判断是否有可用的GPU，如果有，则将设备设置为”cuda”，否则设置为”cpu”。然后打印出当前设备。<br>接下来，代码使用一个循环来进行交叉验证。对于每个折叠（fold），代码会初始化一个模型，并将其移动到指定的设备上。同时，还会初始化损失函数（使用Focal Loss）和优化器（使用Adam优化器），以及一些用于记录训练过程的变量。<br>在每个训练周期（epoch）中，代码会将模型设置为训练模式，并遍历训练数据集。对于每个批次（batch），代码会前向传播数据，计算损失和准确率，并进行反向传播和参数更新。同时，还会记录训练过程中的损失和准确率。<br>在每个验证周期（epoch）中，代码会将模型设置为评估模式，并遍历验证数据集。对于每个批次，代码会前向传播数据，计算损失和准确率，并记录下来。<br>在每个验证周期结束后，代码会计算整个验证集的平均损失和准确率，并与之前的最佳准确率进行比较。如果当前准确率更好，则保存模型，并更新最佳准确率。如果连续若干个验证周期都没有提升，则提前停止训练。<br>整个训练和验证过程会在每个折叠上进行，最终得到的模型是在所有折叠上训练得到的最佳模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &quot;cuda&quot; only when GPUs are available.</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"></span><br><span class="line">test_fold = k_fold</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(test_fold):</span><br><span class="line">    fold = i+<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;\n\nStarting Fold: <span class="subst">&#123;fold&#125;</span> ********************************************&#x27;</span>)</span><br><span class="line">    model = Classifier(Residual_Block, num_layers).to(device)</span><br><span class="line">    criterion = FocalLoss(<span class="number">11</span>, alpha=alpha)</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.0004</span>, weight_decay=<span class="number">1e-5</span>) </span><br><span class="line">    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=<span class="number">16</span>, T_mult=<span class="number">1</span>)</span><br><span class="line">    stale = <span class="number">0</span></span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    val_data = total_files[i*num: (i+<span class="number">1</span>)*num]</span><br><span class="line">    train_data = total_files[:i*num] + total_files[(i+<span class="number">1</span>)*num:]</span><br><span class="line">    </span><br><span class="line">    train_set = FoodDataset(tfm=train_tfm, files=train_data)</span><br><span class="line">    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    valid_set = FoodDataset(tfm=test_tfm, files=val_data)</span><br><span class="line">    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ---------- Training ----------</span></span><br><span class="line">        <span class="comment"># Make sure the model is in train mode before training.</span></span><br><span class="line">        model.train()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># These are used to record information in training.</span></span><br><span class="line">        train_loss = []</span><br><span class="line">        train_accs = []</span><br><span class="line">        lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">        </span><br><span class="line">        pbar = tqdm(train_loader)</span><br><span class="line">        pbar.set_description(<span class="string">f&#x27;T: <span class="subst">&#123;epoch+<span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;n_epochs:03d&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> pbar:</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># A batch consists of image data and corresponding labels.</span></span><br><span class="line">            imgs, labels = batch</span><br><span class="line">            <span class="comment"># Calculate the cross-entropy loss.</span></span><br><span class="line">            <span class="comment"># We don&#x27;t need to apply softmax before computing cross-entropy as it is done automatically.</span></span><br><span class="line">            loss = criterion(logits, labels.to(device))</span><br><span class="line">            <span class="comment"># Gradients stored in the parameters in the previous step should be cleared out first.</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># Compute the gradients for parameters.</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># Clip the gradient norms for stable training.</span></span><br><span class="line">            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">10</span>)</span><br><span class="line">            <span class="comment"># Update the parameters with computed gradients.</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># Compute the accuracy for current batch.</span></span><br><span class="line">            acc = (logits.argmax(dim=-<span class="number">1</span>) == labels.to(device)).<span class="built_in">float</span>().mean()</span><br><span class="line">            <span class="comment"># Record the loss and accuracy.</span></span><br><span class="line">            train_loss.append(loss.item())</span><br><span class="line">            train_accs.append(acc)</span><br><span class="line">            pbar.set_postfix(&#123;<span class="string">&#x27;lr&#x27;</span>:lr, <span class="string">&#x27;b_loss&#x27;</span>:loss.item(), <span class="string">&#x27;b_acc&#x27;</span>:acc.item(),</span><br><span class="line">                    <span class="string">&#x27;loss&#x27;</span>:<span class="built_in">sum</span>(train_loss)/<span class="built_in">len</span>(train_loss), <span class="string">&#x27;acc&#x27;</span>: <span class="built_in">sum</span>(train_accs).item()/<span class="built_in">len</span>(train_accs)&#125;)</span><br><span class="line">        </span><br><span class="line">        scheduler.step()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># These are used to record information in validation.</span></span><br><span class="line">        valid_loss = []</span><br><span class="line">        valid_accs = []</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Iterate the validation set by batches.</span></span><br><span class="line">        pbar = tqdm(valid_loader)</span><br><span class="line">        pbar.set_description(<span class="string">f&#x27;V: <span class="subst">&#123;epoch+<span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;n_epochs:03d&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> pbar:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># A batch consists of image data and corresponding labels.</span></span><br><span class="line">            imgs, labels = batch</span><br><span class="line">  </span><br><span class="line">            <span class="comment"># We don&#x27;t need gradient in validation.</span></span><br><span class="line">            <span class="comment"># Using torch.no_grad() accelerates the forward process.</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                logits = model(imgs.to(device))</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># We can still compute the loss (but not the gradient).</span></span><br><span class="line">            loss = criterion(logits, labels.to(device))</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Compute the accuracy for current batch.</span></span><br><span class="line">            acc = (logits.argmax(dim=-<span class="number">1</span>) == labels.to(device)).<span class="built_in">float</span>().mean()</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Record the loss and accuracy.</span></span><br><span class="line">            valid_loss.append(loss.item())</span><br><span class="line">            valid_accs.append(acc)</span><br><span class="line">            pbar.set_postfix(&#123;<span class="string">&#x27;v_loss&#x27;</span>:<span class="built_in">sum</span>(valid_loss)/<span class="built_in">len</span>(valid_loss), </span><br><span class="line">                              <span class="string">&#x27;v_acc&#x27;</span>: <span class="built_in">sum</span>(valid_accs).item()/<span class="built_in">len</span>(valid_accs)&#125;)</span><br><span class="line">        </span><br><span class="line">            <span class="comment">#break</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># The average loss and accuracy for entire validation set is the average of the recorded values.</span></span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(valid_loss) / <span class="built_in">len</span>(valid_loss)</span><br><span class="line">        valid_acc = <span class="built_in">sum</span>(valid_accs) / <span class="built_in">len</span>(valid_accs)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> valid_acc &gt; best_acc:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Best model found at fold <span class="subst">&#123;fold&#125;</span> epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, acc=<span class="subst">&#123;valid_acc:<span class="number">.5</span>f&#125;</span>, saving model&quot;</span>)</span><br><span class="line">            torch.save(model.state_dict(), <span class="string">f&quot;Fold_<span class="subst">&#123;fold&#125;</span>_best.ckpt&quot;</span>)</span><br><span class="line">            <span class="comment"># only save best to prevent output memory exceed error</span></span><br><span class="line">            best_acc = valid_acc</span><br><span class="line">            stale = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            stale += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> stale &gt; patience:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;No improvment <span class="subst">&#123;patience&#125;</span> consecutive epochs, early stopping&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<h3 id="测试并生成预测表"><a href="#测试并生成预测表" class="headerlink" title="测试并生成预测表"></a>测试并生成预测表</h3><p>最后对测试数据进行预测。<br>首先，代码创建了一个空列表models，用于存储每个折叠上的最佳模型。<br>然后，代码使用一个循环，对每个折叠进行操作。在每个折叠中，代码会初始化一个模型model_best，并加载之前保存的最佳模型的参数。然后，将模型设置为评估模式。最后，将模型添加到models列表中。<br>接下来，代码使用torch.no_grad()禁用梯度计算。然后，对于测试数据集中的每个批次，代码会使用每个折叠上的模型进行预测。预测结果会进行累加，并通过np.argmax函数找到每个样本的最大预测值对应的类别。最后，将预测结果添加到prediction列表中。<br>最终，prediction列表中存储了所有测试样本的预测结果。<br>最后创建测试集的CSV文件，以便提交预测结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">models = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(test_fold):</span><br><span class="line">    fold = i + <span class="number">1</span></span><br><span class="line">    model_best = Classifier(Residual_Block, num_layers).to(device)</span><br><span class="line">    model_best.load_state_dict(torch.load(<span class="string">f&quot;Fold_<span class="subst">&#123;fold&#125;</span>_best.ckpt&quot;</span>))</span><br><span class="line">    model_best.<span class="built_in">eval</span>()</span><br><span class="line">    models.append(model_best)</span><br><span class="line"></span><br><span class="line">prediction = []            </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data,_ <span class="keyword">in</span> test_loader:</span><br><span class="line">        test_preds = [] </span><br><span class="line">        <span class="keyword">for</span> model_best <span class="keyword">in</span> models:</span><br><span class="line">            test_preds.append(model_best(data.to(device)).cpu().data.numpy())</span><br><span class="line">        test_preds = <span class="built_in">sum</span>(test_preds)</span><br><span class="line">        test_label = np.argmax(test_preds, axis=<span class="number">1</span>)</span><br><span class="line">        prediction += test_label.squeeze().tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment">#create test csv</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad4</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;0&quot;</span>*(<span class="number">4</span>-<span class="built_in">len</span>(<span class="built_in">str</span>(i)))+<span class="built_in">str</span>(i)</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&quot;Id&quot;</span>] = [pad4(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(test_set)+<span class="number">1</span>)]</span><br><span class="line">df[<span class="string">&quot;Category&quot;</span>] = prediction</span><br><span class="line">df.to_csv(<span class="string">&quot;submission.csv&quot;</span>,index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/04/frame/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/04/frame/" class="post-title-link" itemprop="url">音位分类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-04 12:51:32 / 修改时间：15:42:32" itemprop="dateCreated datePublished" datetime="2023-08-04T12:51:32+08:00">2023-08-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在本次作业中，正常的一段音频素材可能包含大量的<strong>音位</strong>信息，而音位之间又可能存在重叠干扰的情况，因此我们将一段音频素材每隔10ms切取25ms，以此来尽可能保存完整的音位素材，取出后的素材称为一个frame，取出后的frame并不适合直接进入训练，因此我们要进行进一步的处理，通过MFCC，将它转化为一个39维度的特征，转换后为了更加精确的判断当前特征内的音位信息，我们往往采取其前后的特征来做辅助判断 也就是前向特征与后向特征各取5个，所以我们最后得到的是一个11*39维的一个向量。</p>
<h3 id="概念解释"><a href="#概念解释" class="headerlink" title="概念解释"></a>概念解释</h3><p>音位：(phonetics 语音) 音位，音素（区分单词的最小语音单位，英语sip中的s和zip中的z是两个不同的音素）<br>MFCC：在语音识别（Speech recognition）和话者识别（Speaker recognition）方面，最常用到的语音特征就是梅尔倒谱系数（Mel-scale Frequency Cepstral Coefficients, MFCC）。</p>
<h3 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h3><p>需要处理的数据有三个txt文件和一个frame文件夹，后者储存了音频文件的训练集与测试集。<br>train_split.txt: 其中每一行对应一个训练数据，其所对应的文件在feat/train/中；<br>train_labels.txt: 由训练数据和labels组成，格式为: filename labels。其中，label 为 frame 对应的 phoneme；<br>test_split.txt: 其中每一行对应一个训练数据，其所对应的文件在feat/test/中；<br>feat/train/{id}.pt 和 feat/test/{id}.pt: 音频对应的 MFCC w/ CMVN，维度为39，这些文件可以通过torch.load()直接导入，导入后的shape为(T, 39)。</p>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>首先导入各种库，准备四个处理数据的函数。<br>load_feat函数用于加载特征文件，使用torch.load函数加载路径下的文件，并将加载的结果返回。<br>shift函数用于对特征进行平移操作，它接受两个参数：x为输入的特征向量，n为平移的步数。如果n小于0，则将x的第一个元素重复-n次作为左边的填充，将x的前n个元素作为右边的填充；如果n大于0，则将x的最后一个元素重复n次作为右边的填充，将x的后n个元素作为左边的填充；如果n等于0，则直接返回x。<br>concat_feat函数用于对特征进行拼接操作，它接受两个参数：x为输入的特征矩阵，concat_n为要拼接的帧数。<br>preprocess_data函数用于数据预处理，函数中首先定义了一个class_num变量，表示类别数量。然后根据split确定mode（训练集、验证集或测试集）。接下来根据split划分数据集。然后遍历usage_list中的每个文件名，加载对应的特征文件，将其进行拼接，并将拼接后的特征存储在X中。如果不是测试集，还会将对应的标签存储在y中。最后根据实际使用的长度截取X和y，并打印相关信息。如果是测试集，只返回X。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_feat</span>(<span class="params">path</span>):</span><br><span class="line">    feat = torch.load(path)</span><br><span class="line">    <span class="keyword">return</span> feat</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift</span>(<span class="params">x, n</span>):</span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">0</span>:</span><br><span class="line">        left = x[<span class="number">0</span>].repeat(-n, <span class="number">1</span>)</span><br><span class="line">        right = x[:n]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        right = x[-<span class="number">1</span>].repeat(n, <span class="number">1</span>)</span><br><span class="line">        left = x[n:]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.cat((left, right), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">concat_feat</span>(<span class="params">x, concat_n</span>):</span><br><span class="line">    <span class="keyword">assert</span> concat_n % <span class="number">2</span> == <span class="number">1</span> <span class="comment"># n must be odd</span></span><br><span class="line">    <span class="keyword">if</span> concat_n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    seq_len, feature_dim = x.size(<span class="number">0</span>), x.size(<span class="number">1</span>)</span><br><span class="line">    x = x.repeat(<span class="number">1</span>, concat_n) </span><br><span class="line">    x = x.view(seq_len, concat_n, feature_dim).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># concat_n, seq_len, feature_dim</span></span><br><span class="line">    mid = (concat_n // <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> r_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, mid+<span class="number">1</span>):</span><br><span class="line">        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)</span><br><span class="line">        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).view(seq_len, concat_n * feature_dim)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">split, feat_dir, phone_path, concat_nframes, train_ratio=<span class="number">0.8</span>, train_val_seed=<span class="number">1337</span></span>):</span><br><span class="line">    class_num = <span class="number">41</span> <span class="comment"># <span class="doctag">NOTE:</span> pre-computed, should not need change</span></span><br><span class="line">    mode = <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> (split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">or</span> split == <span class="string">&#x27;val&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;test&#x27;</span></span><br><span class="line"></span><br><span class="line">    label_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">      phone_file = <span class="built_in">open</span>(os.path.join(phone_path, <span class="string">f&#x27;<span class="subst">&#123;mode&#125;</span>_labels.txt&#x27;</span>)).readlines()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> line <span class="keyword">in</span> phone_file:</span><br><span class="line">          line = line.strip(<span class="string">&#x27;\n&#x27;</span>).split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">          label_dict[line[<span class="number">0</span>]] = [<span class="built_in">int</span>(p) <span class="keyword">for</span> p <span class="keyword">in</span> line[<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">or</span> split == <span class="string">&#x27;val&#x27;</span>:</span><br><span class="line">        <span class="comment"># split training and validation data</span></span><br><span class="line">        usage_list = <span class="built_in">open</span>(os.path.join(phone_path, <span class="string">&#x27;train_split.txt&#x27;</span>)).readlines()</span><br><span class="line">        random.seed(train_val_seed)</span><br><span class="line">        random.shuffle(usage_list)</span><br><span class="line">        percent = <span class="built_in">int</span>(<span class="built_in">len</span>(usage_list) * train_ratio)</span><br><span class="line">        usage_list = usage_list[:percent] <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> usage_list[percent:]</span><br><span class="line">    <span class="keyword">elif</span> split == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        usage_list = <span class="built_in">open</span>(os.path.join(phone_path, <span class="string">&#x27;test_split.txt&#x27;</span>)).readlines()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid \&#x27;split\&#x27; argument for dataset: PhoneDataset!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    usage_list = [line.strip(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">for</span> line <span class="keyword">in</span> usage_list]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;[Dataset] - # phone classes: &#x27;</span> + <span class="built_in">str</span>(class_num) + <span class="string">&#x27;, number of utterances for &#x27;</span> + split + <span class="string">&#x27;: &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(usage_list)))</span><br><span class="line"></span><br><span class="line">    max_len = <span class="number">3000000</span></span><br><span class="line">    X = torch.empty(max_len, <span class="number">39</span> * concat_nframes)</span><br><span class="line">    <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">      y = torch.empty(max_len, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(usage_list)):</span><br><span class="line">        feat = load_feat(os.path.join(feat_dir, mode, <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.pt&#x27;</span>))</span><br><span class="line">        cur_len = <span class="built_in">len</span>(feat)</span><br><span class="line">        feat = concat_feat(feat, concat_nframes)</span><br><span class="line">        <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">          label = torch.LongTensor(label_dict[fname])</span><br><span class="line"></span><br><span class="line">        X[idx: idx + cur_len, :] = feat</span><br><span class="line">        <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">          y[idx: idx + cur_len] = label</span><br><span class="line"></span><br><span class="line">        idx += cur_len</span><br><span class="line"></span><br><span class="line">    X = X[:idx, :]</span><br><span class="line">    <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">      y = y[:idx]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;[INFO] <span class="subst">&#123;split&#125;</span> set&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="keyword">if</span> mode != <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">      <span class="built_in">print</span>(y.shape)</span><br><span class="line">      <span class="keyword">return</span> X, y</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure></p>
<h3 id="数据集定义"><a href="#数据集定义" class="headerlink" title="数据集定义"></a>数据集定义</h3><p>这是一个用于创建数据集的类。它接受输入数据X和可选的标签y，并将它们存储在self.data和self.label中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LibriDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        self.data = X</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.label = torch.LongTensor(y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.label = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">if</span> self.label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure></p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>基础模型定义中依旧包含了上次作业中采用的批量归一化层和Dropout层。<br>Classifier类是一个分类器模型，它由多个BasicBlock组成。它的输入维度为input_dim，输出维度为output_dim。hidden_layers参数指定了隐藏层的数量，hidden_dim参数指定了隐藏层的维度。在forward方法中，输入数据经过多个BasicBlock后，最终通过一个线性层映射到output_dim维度，得到分类结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.block = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, output_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.BatchNorm1d(output_dim),</span><br><span class="line">            nn.Dropout(<span class="number">0.35</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.block(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim=<span class="number">41</span>, hidden_layers=<span class="number">1</span>, hidden_dim=<span class="number">256</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Classifier, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            BasicBlock(input_dim, hidden_dim),</span><br><span class="line">            *[BasicBlock(hidden_dim, hidden_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(hidden_layers)],</span><br><span class="line">            nn.Linear(hidden_dim, output_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="超参数定义"><a href="#超参数定义" class="headerlink" title="超参数定义"></a>超参数定义</h3><p>在参考代码上做了一些调整，对train_labels.txt文件进行统计，发现每一个音位占用的frame均值是9个，因此可以将concat_nframes参数设置为&gt;9（必须为奇数），经尝试可以将concat_nframes设置的大些，这里我设置为19。网络架构调整的更宽和稍深。batch_size设置为2048。设置三个宽度为1024的隐藏层。学习率也可以稍微调整的大些。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data prarameters</span></span><br><span class="line">concat_nframes = <span class="number">19</span>            <span class="comment"># the number of frames to concat with, n must be odd (total 2k+1 = n frames)</span></span><br><span class="line">train_ratio = <span class="number">0.8</span>               <span class="comment"># the ratio of data used for training, the rest will be used for validation</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training parameters</span></span><br><span class="line">seed = <span class="number">0</span>                        <span class="comment"># random seed</span></span><br><span class="line">batch_size = <span class="number">2048</span>                <span class="comment"># batch size</span></span><br><span class="line">num_epoch = <span class="number">50</span>                   <span class="comment"># the number of training epoch</span></span><br><span class="line">early_stopping = <span class="number">8</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span>            <span class="comment">#learning rate</span></span><br><span class="line">model_path = <span class="string">&#x27;./model.ckpt&#x27;</span>     <span class="comment"># the path where the checkpoint will be saved</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model parameters</span></span><br><span class="line">input_dim = <span class="number">39</span> * concat_nframes <span class="comment"># the input dim of the model, you should not change the value</span></span><br><span class="line">hidden_layers = <span class="number">3</span>              <span class="comment"># the number of hidden layers</span></span><br><span class="line">hidden_dim = <span class="number">1024</span>              <span class="comment"># the hidden dim</span></span><br></pre></td></tr></table></figure></p>
<h3 id="数据集与模型加载"><a href="#数据集与模型加载" class="headerlink" title="数据集与模型加载"></a>数据集与模型加载</h3><p>首先，preprocess_data函数对数据进行预处理。split参数指定了数据集的划分（训练集或验证集），feat_dir参数指定了特征文件的路径，phone_path参数指定了标签文件的路径，concat_nframes参数指定了特征拼接的帧数，train_ratio参数指定了训练集的比例。预处理后得到训练集的特征数据train_X和标签数据train_y，以及验证集的特征数据val_X和标签数据val_y。<br>然后，通过LibriDataset类将特征数据和标签数据封装成数据集对象。train_set表示训练集数据集，val_set表示验证集数据集。<br>接下来，为了释放内存，使用del语句删除了train_X、train_y、val_X、val_y这些不再需要的变量，并通过gc.collect()回收垃圾。<br>最后，通过DataLoader类将数据集对象转换成数据加载器对象。train_loader表示训练集的数据加载器，val_loader表示验证集的数据加载器。batch_size参数指定了每个批次的样本数量，shuffle参数指定了是否对数据进行洗牌。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocess data</span></span><br><span class="line">train_X, train_y = preprocess_data(split=<span class="string">&#x27;train&#x27;</span>, feat_dir=<span class="string">&#x27;./libriphone/feat&#x27;</span>, phone_path=<span class="string">&#x27;./libriphone&#x27;</span>, concat_nframes=concat_nframes, train_ratio=train_ratio)</span><br><span class="line">val_X, val_y = preprocess_data(split=<span class="string">&#x27;val&#x27;</span>, feat_dir=<span class="string">&#x27;./libriphone/feat&#x27;</span>, phone_path=<span class="string">&#x27;./libriphone&#x27;</span>, concat_nframes=concat_nframes, train_ratio=train_ratio)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get dataset</span></span><br><span class="line">train_set = LibriDataset(train_X, train_y)</span><br><span class="line">val_set = LibriDataset(val_X, val_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># remove raw feature to save memory</span></span><br><span class="line"><span class="keyword">del</span> train_X, train_y, val_X, val_y</span><br><span class="line">gc.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get dataloader</span></span><br><span class="line">train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Dataset] - <span class="comment"># phone classes: 41, number of utterances for train: 3428</span></span><br><span class="line">3428it [00:16, 213.27it/s]</span><br><span class="line">[INFO] train <span class="built_in">set</span></span><br><span class="line">torch.Size([2116368, 741])</span><br><span class="line">torch.Size([2116368])</span><br><span class="line">[Dataset] - <span class="comment"># phone classes: 41, number of utterances for val: 858</span></span><br><span class="line">858it [00:04, 198.50it/s]</span><br><span class="line">[INFO] val <span class="built_in">set</span></span><br><span class="line">torch.Size([527790, 741])</span><br><span class="line">torch.Size([527790])</span><br></pre></td></tr></table></figure><br>设备设为GPU。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;DEVICE: <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEVICE: cuda:0</span><br></pre></td></tr></table></figure><br>首先利用same_seeds函数确定结果可复现。<br>然后，通过Classifier类创建了一个模型对象model。<br>接下来，定义了损失函数criterion。这里使用了交叉熵损失函数，用于多分类任务。<br>然后，定义了优化器optimizer。使用了AdamW优化器，并将模型的参数传递给优化器。lr参数指定了学习率为原参数的5倍，weight_decay参数指定了权重衰减（0.01）。<br>最后，定义了学习率调度器scheduler。使用了余弦退火重启调度器，并将优化器和相关参数传递给调度器。这个调度器可以在训练过程中动态地调整学习率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#fix seed</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">same_seeds</span>(<span class="params">seed</span>):</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)  </span><br><span class="line">    np.random.seed(seed)  </span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fix random seed</span></span><br><span class="line">same_seeds(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model, define a loss function, and optimizer</span></span><br><span class="line">model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss() </span><br><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate*<span class="number">5</span>, weight_decay=<span class="number">0.01</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, </span><br><span class="line">                                        T_0=<span class="number">8</span>, T_mult=<span class="number">2</span>, eta_min=learning_rate/<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>下面这段代码是模型的训练和验证过程。<br>首先，定义了初始的最佳准确率best_acc和早停计数器early_stop_count。<br>然后，进入每个训练轮次的循环。在每个训练轮次中，初始化训练准确率train_acc和训练损失train_loss，验证准确率val_acc和验证损失val_loss。<br>接下来，将模型设置为训练模式（model.train()）。使用tqdm库创建一个进度条pbar，用于显示训练进度。对于每个批次（batch）数据，进行前向传播、计算损失、反向传播和优化器更新。同时计算训练准确率和训练损失，并更新进度条的显示。最后，使用学习率调度器更新学习率，并关闭进度条。<br>接下来，如果存在验证集（len(val_set) &gt; 0），则进入验证阶段。将模型设置为评估模式（model.eval()），并使用torch.no_grad()关闭梯度计算。同样使用tqdm库创建一个进度条pbar，用于显示验证进度。对于每个批次数据，将特征数据和标签数据移动到指定的设备上，并进行前向传播和计算损失。同时计算验证准确率和验证损失，并更新进度条的显示。最后，如果验证准确率超过了最佳准确率best_acc，则保存模型的权重，并打印出当前最佳准确率。如果验证准确率没有提升，则增加早停计数器early_stop_count。如果早停计数器达到了早停次数（early_stopping），则打印出早停信息并跳出训练轮次的循环。<br>最后，如果没有验证集（len(val_set) == 0），则直接保存最后一个训练轮次的模型权重。<br>总结起来，这段代码实现了模型的训练和验证过程，并在验证准确率提升时保存模型的权重，如果验证准确率连续多次没有提升则进行早停。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">best_acc = <span class="number">0.0</span></span><br><span class="line">early_stop_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    train_acc = <span class="number">0.0</span></span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    val_loss = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    model.train() <span class="comment"># set the model to training mode</span></span><br><span class="line">    pbar = tqdm(train_loader, ncols=<span class="number">110</span>)</span><br><span class="line">    pbar.set_description(<span class="string">f&#x27;T: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epoch&#125;</span>&#x27;</span>)</span><br><span class="line">    samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(pbar):</span><br><span class="line">        features, labels = batch</span><br><span class="line">        features = features.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad() </span><br><span class="line">        outputs = model(features) </span><br><span class="line">        </span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward() </span><br><span class="line">        optimizer.step()</span><br><span class="line">       </span><br><span class="line">        </span><br><span class="line">        _, train_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment"># get the index of the class with the highest probability</span></span><br><span class="line">        correct = (train_pred.detach() == labels.detach()).<span class="built_in">sum</span>().item()</span><br><span class="line">        train_acc += correct</span><br><span class="line">        samples += labels.size(<span class="number">0</span>)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">        pbar.set_postfix(&#123;<span class="string">&#x27;lr&#x27;</span>:lr, <span class="string">&#x27;batch acc&#x27;</span>:correct/labels.size(<span class="number">0</span>),</span><br><span class="line">                          <span class="string">&#x27;acc&#x27;</span>:train_acc/samples, <span class="string">&#x27;loss&#x27;</span>:train_loss/(i+<span class="number">1</span>)&#125;)</span><br><span class="line">    scheduler.step()</span><br><span class="line">    pbar.close()</span><br><span class="line">    <span class="comment"># validation</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(val_set) &gt; <span class="number">0</span>:</span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># set the model to evaluation mode</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            pbar = tqdm(val_loader, ncols=<span class="number">110</span>)</span><br><span class="line">            pbar.set_description(<span class="string">f&#x27;V: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epoch&#125;</span>&#x27;</span>)</span><br><span class="line">            samples = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(pbar):</span><br><span class="line">                features, labels = batch</span><br><span class="line">                features = features.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                outputs = model(features)</span><br><span class="line">                </span><br><span class="line">                loss = criterion(outputs, labels) </span><br><span class="line">                </span><br><span class="line">                _, val_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment">#get the index of the class with the highest probability</span></span><br><span class="line">                val_acc += (val_pred.cpu() == labels.cpu()).<span class="built_in">sum</span>().item()</span><br><span class="line">                samples += labels.size(<span class="number">0</span>)</span><br><span class="line">                val_loss += loss.item()</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&#x27;val acc&#x27;</span>:val_acc/samples ,<span class="string">&#x27;val loss&#x27;</span>:val_loss/(i+<span class="number">1</span>)&#125;)</span><br><span class="line">            pbar.close()</span><br><span class="line">            <span class="comment"># if the model improves, save a checkpoint at this epoch</span></span><br><span class="line">        <span class="keyword">if</span> val_acc &gt; best_acc:</span><br><span class="line">            best_acc = val_acc</span><br><span class="line">            torch.save(model.state_dict(), model_path)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;saving model with acc &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc/<span class="built_in">len</span>(val_set)))</span><br><span class="line">            early_stop_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            early_stop_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> early_stop_count &gt;= early_stopping:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, model not improving, early stopping.&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch+<span class="number">1</span>:03d&#125;</span>/<span class="subst">&#123;num_epoch:03d&#125;</span>] Acc: <span class="subst">&#123;acc:<span class="number">3.6</span>f&#125;</span> Loss: <span class="subst">&#123;loss:<span class="number">3.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if not validating, save the last epoch</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(val_set) == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.state_dict(), model_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;saving model at last epoch&#x27;</span>)</span><br></pre></td></tr></table></figure><br>训练和验证过程之后，及时释放不再需要的数据加载器对象，以减少内存占用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> train_loader, val_loader</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>加载测试集数据和模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">test_X = preprocess_data(split=<span class="string">&#x27;test&#x27;</span>, feat_dir=<span class="string">&#x27;./libriphone/feat&#x27;</span>, phone_path=<span class="string">&#x27;./libriphone&#x27;</span>, concat_nframes=concat_nframes)</span><br><span class="line">test_set = LibriDataset(test_X, <span class="literal">None</span>)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># load model</span></span><br><span class="line">model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)</span><br><span class="line">model.load_state_dict(torch.load(model_path))</span><br></pre></td></tr></table></figure><br>接下来对测试集进行预测。<br>首先，初始化测试准确率test_acc和测试样本数量test_lengths，以及一个空的预测结果数组pred。<br>然后，将模型设置为评估模式（model.eval()），并关闭梯度计算。使用tqdm库创建一个进度条，用于显示测试进度。对每个批次的测试数据进行前向传播。使用torch.max函数获取输出中概率最高的类别索引test_pred，并将其转换为numpy数组，并将其与之前的预测结果数组pred进行拼接。<br>最后，完成所有测试数据的预测后，可以使用pred数组进行后续的评估和分析。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">test_acc = <span class="number">0.0</span></span><br><span class="line">test_lengths = <span class="number">0</span></span><br><span class="line">pred = np.array([], dtype=np.int32)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(test_loader)):</span><br><span class="line">        features = batch</span><br><span class="line">        features = features.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = model(features)</span><br><span class="line"></span><br><span class="line">        _, test_pred = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment"># get the index of the class with the highest probability</span></span><br><span class="line">        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><br>保存预测数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;prediction.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;Id,Class\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, y <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred):</span><br><span class="line">        f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(i, y))</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/02/Covid-19-Pred/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/02/Covid-19-Pred/" class="post-title-link" itemprop="url">COVID-19 病例预测 (回归)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-02 14:51:44" itemprop="dateCreated datePublished" datetime="2023-08-02T14:51:44+08:00">2023-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-08-04 16:00:57" itemprop="dateModified" datetime="2023-08-04T16:00:57+08:00">2023-08-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学习目标:<br>模型拟合：用深度神经网络 (DNN)拟合一个回归模型。<br>掌握技巧：<br>熟悉并掌握DNN模型训练的基本技巧。<br>提高PyTorch的使用熟练度。</p>
<h3 id="Import-packages"><a href="#Import-packages" class="headerlink" title="Import packages"></a>Import packages</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numerical Operations</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading/Writing Data</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># For Progress Bar</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pytorch</span></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader, random_split</span><br></pre></td></tr></table></figure>
<h3 id="Some-Utility-Functions"><a href="#Some-Utility-Functions" class="headerlink" title="Some Utility Functions"></a>Some Utility Functions</h3><p>same_seed函数用于复现实验结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">same_seed</span>(<span class="params">seed</span>): </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Fixes random number generator seeds for reproducibility.&#x27;&#x27;&#x27;</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure><br>下面的函数用于将提供的训练数据分割成训练集和验证集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_valid_split</span>(<span class="params">data_set, valid_ratio, seed</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Split provided training data into training set and validation set&#x27;&#x27;&#x27;</span></span><br><span class="line">    valid_set_size = <span class="built_in">int</span>(valid_ratio * <span class="built_in">len</span>(data_set)) </span><br><span class="line">    train_set_size = <span class="built_in">len</span>(data_set) - valid_set_size</span><br><span class="line">    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size],</span><br><span class="line">                                        generator=torch.Generator().manual_seed(seed))</span><br><span class="line">    <span class="keyword">return</span> np.array(train_set), np.array(valid_set)</span><br></pre></td></tr></table></figure><br>下面函数用于对测试集进行预测。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">test_loader, model, device</span>): </span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># Set your model to evaluation mode.</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> tqdm(test_loader):</span><br><span class="line">        x = x.to(device)                        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():                   </span><br><span class="line">            pred = model(x)                     </span><br><span class="line">            preds.append(pred.detach().cpu())   </span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">0</span>).numpy()  </span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure></p>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>这个类是一个自定义的数据集类，用于加载COVID-19数据集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">COVID19Dataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    x: Features.</span></span><br><span class="line"><span class="string">    y: Targets, if none, do prediction.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.y = y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.y = torch.FloatTensor(y)</span><br><span class="line">        self.x = torch.FloatTensor(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">if</span> self.y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.x[idx]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br></pre></td></tr></table></figure></p>
<h3 id="Neural-Network-Model"><a href="#Neural-Network-Model" class="headerlink" title="Neural Network Model"></a>Neural Network Model</h3><p>下面的类是一个自定义的模型类，用于构建一个神经网络模型，继承自torch.nn.Module类。在这个方法中，我们可以修改模型的结构。<br><strong>init</strong>方法用于初始化模型对象。根据给定的特征数量，构建了一个包含多个线性层（nn.Linear）、激活函数（nn.LeakyReLU）、批归一化层（nn.BatchNorm1d）和dropout层（nn.Dropout）的序列模型（nn.Sequential）。<br>其中，torch.nn.Dropout(num)是一种为了防止训练模型过拟合的方法。通过丢弃num比例的隐藏层神经元，不参与训练，可以有效的防止过拟合。<br>torch.nn.BatchNorm1d()是为了保持深度神经网络训练过程中每一层神经网络的输入同分布的方法。训练深度网络的时候经常发生训练困难的问题：因为，每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难，这被称为Internal Covariate Shift。为了解决Internal Covariate Shift，我们使用Batch Normalization。<br>forward方法用于定义模型的前向传播过程。在这个方法中，我们将输入数据传递给模型的层（self.layers）进行计算。最后，我们将输出进行压缩，从形状为(B, 1)的张量变为形状为(B)的张量，并将其返回。<br>通过实现这个模型类，我们可以实例化一个模型对象，并将其用于训练和预测。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">My_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(My_Model, self).__init__()</span><br><span class="line">        <span class="comment">#input_dim is the num of features we selected</span></span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">64</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">64</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line">            </span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">16</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            <span class="comment">#nn.BatchNorm1d(10),</span></span><br><span class="line">            nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line">            </span><br><span class="line">            nn.Linear(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.squeeze(<span class="number">1</span>) <span class="comment"># (B, 1) -&gt; (B)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>特征工程部分使用了sklearn库中的特征选择方法来选择COVID-19数据集中的最佳特征。<br>首先，从”covid.train.csv”文件中读取特征数据，并将其分为输入特征（x_data）和目标（y_data）。<br>接下来，定义了一个特征选择器（selector）对象，使用f_regression作为评分函数，并指定选择的特征数量（k）。然后，使用fit方法将输入特征（x_data）和目标（y_data）传递给特征选择器进行拟合。<br>通过result.scores_可以获取每个特征的得分，通过np.argsort对得分进行排序，并使用[::-1]将排序结果逆序，得到了得分从高到低的特征索引（idx）。<br>接着，打印出得分最高的k个特征的得分、索引和名称。<br>最后，将得分最高的k个特征的索引（idx[:k]）进行排序，并打印出对应的特征名称。<br>通过这段代码，我们可以获得COVID-19数据集中得分最高的k个特征，并将其索引和名称打印出来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line">features = pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>)</span><br><span class="line">x_data, y_data = features.iloc[:, <span class="number">0</span>:<span class="number">117</span>], features.iloc[:, <span class="number">117</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#choose k best features</span></span><br><span class="line">k = <span class="number">24</span></span><br><span class="line">selector = SelectKBest(score_func=f_regression, k=k)</span><br><span class="line">result = selector.fit(x_data, y_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#result.scores_ inclues scores for each features</span></span><br><span class="line"><span class="comment">#np.argsort sort scores in ascending order by index, we reverse it to make it descending.</span></span><br><span class="line">idx = np.argsort(result.scores_)[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Top <span class="subst">&#123;k&#125;</span> Best feature score &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result.scores_[idx[:k]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nTop <span class="subst">&#123;k&#125;</span> Best feature index &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(idx[:k])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\nTop <span class="subst">&#123;k&#125;</span> Best feature name&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_data.columns[idx[:k]])</span><br><span class="line"></span><br><span class="line">selected_idx = <span class="built_in">list</span>(np.sort(idx[:k]))</span><br><span class="line"><span class="built_in">print</span>(selected_idx)</span><br><span class="line"><span class="built_in">print</span>(x_data.columns[selected_idx])</span><br></pre></td></tr></table></figure><br>输出<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Top 24 Best feature score </span><br><span class="line">[90072.43401367 42336.37370139 26889.70377033 18870.55811361</span><br><span class="line"> 11290.79919656 10849.62638725 10420.334481   10365.26105926</span><br><span class="line"> 10055.85024148  9859.62690961  9636.4254885   9330.74236337</span><br><span class="line">  9180.16305651  8703.90128488  7857.10815311  7840.26399997</span><br><span class="line">  7669.80626316  7634.02832221  7471.14168359  7427.81602428</span><br><span class="line">  7379.57519499  7350.43226583  7200.10012107  7189.72334257]</span><br><span class="line"></span><br><span class="line">Top 24 Best feature index </span><br><span class="line">[101  85  69  53 104  88 105  72  89  56  73  40  57  41 103 102  87  86</span><br><span class="line">  71  70  55  54  39  38]</span><br><span class="line"></span><br><span class="line">Top 24 Best feature name</span><br><span class="line">Index([<span class="string">&#x27;tested_positive.3&#x27;</span>, <span class="string">&#x27;tested_positive.2&#x27;</span>, <span class="string">&#x27;tested_positive.1&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;tested_positive&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.4&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.3&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;nohh_cmnty_cli.4&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.2&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli.3&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;hh_cmnty_cli.1&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli.2&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;nohh_cmnty_cli.1&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli&#x27;</span>, <span class="string">&#x27;ili.4&#x27;</span>, <span class="string">&#x27;cli.4&#x27;</span>, <span class="string">&#x27;ili.3&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;cli.3&#x27;</span>, <span class="string">&#x27;ili.2&#x27;</span>, <span class="string">&#x27;cli.2&#x27;</span>, <span class="string">&#x27;ili.1&#x27;</span>, <span class="string">&#x27;cli.1&#x27;</span>, <span class="string">&#x27;ili&#x27;</span>, <span class="string">&#x27;cli&#x27;</span>],</span><br><span class="line">      dtype=<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">[38, 39, 40, 41, 53, 54, 55, 56, 57, 69, 70, 71, 72, 73, 85, 86, 87, 88, 89, 101, 102, 103, 104, 105]</span><br><span class="line">Index([<span class="string">&#x27;cli&#x27;</span>, <span class="string">&#x27;ili&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;cli.1&#x27;</span>, <span class="string">&#x27;ili.1&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.1&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli.1&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;tested_positive.1&#x27;</span>, <span class="string">&#x27;cli.2&#x27;</span>, <span class="string">&#x27;ili.2&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.2&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;nohh_cmnty_cli.2&#x27;</span>, <span class="string">&#x27;tested_positive.2&#x27;</span>, <span class="string">&#x27;cli.3&#x27;</span>, <span class="string">&#x27;ili.3&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;hh_cmnty_cli.3&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli.3&#x27;</span>, <span class="string">&#x27;tested_positive.3&#x27;</span>, <span class="string">&#x27;cli.4&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;ili.4&#x27;</span>, <span class="string">&#x27;hh_cmnty_cli.4&#x27;</span>, <span class="string">&#x27;nohh_cmnty_cli.4&#x27;</span>],</span><br><span class="line">      dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure><br>select_feat函数用于选择有用的特征进行回归分析。它接收训练集、验证集和测试集作为输入，以及一个布尔值参数select_all(是否选取全部特征)。<br>首先，从训练数据和验证数据中分离出目标变量（y_train和y_valid）。<br>然后，将原始的输入特征数据分别赋值给raw_x_train、raw_x_valid和raw_x_test。<br>接下来，根据select_all参数的值来确定是否选择所有特征。如果select_all为True，则feat_idx为包含所有特征索引的列表；如果select_all为False，则feat_idx为之前通过特征选择方法得到的特征索引列表（selected_idx）。<br>最后，函数返回经过特征选择后的训练数据、验证数据、测试数据以及目标变量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_feat</span>(<span class="params">train_data, valid_data, test_data, select_all=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Selects useful features to perform regression&#x27;&#x27;&#x27;</span></span><br><span class="line">    y_train, y_valid = train_data[:,-<span class="number">1</span>], valid_data[:,-<span class="number">1</span>]</span><br><span class="line">    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-<span class="number">1</span>], valid_data[:,:-<span class="number">1</span>], test_data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> select_all:</span><br><span class="line">        feat_idx = <span class="built_in">list</span>(<span class="built_in">range</span>(raw_x_train.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        feat_idx = selected_idx</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid</span><br></pre></td></tr></table></figure></p>
<h3 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h3><p>下面的函数用于训练模型，并返回训练过程中的损失值。<br>首先，定义了损失函数，这里使用均方误差损失。<br>然后，定义了优化算法（optimizer）。在这个函数中，使用了Adam优化算法，设置了50倍的学习率和0.001权重衰减。<br>接下来，定义了学习率调度器（scheduler），使用了余弦退火重启调度器，用于动态调整学习率。<br>在训练过程中，使用了一个循环来遍历每个训练批次。在每个批次中，将梯度置零，利用GPU计算，通过模型进行预测，计算损失，然后进行反向传播，更新参数，并记录损失值。<br>使用tqdm库在训练过程中可视化进度条，并在进度条上显示当前的损失。<br>在每个周期结束后，将模型设置为评估模式，计算验证集上的损失，并记录下来。<br>如果当前的验证损失小于最佳损失，则保存模型，并更新最佳损失。早停机制设置下如果连续early_stop次验证损失没有改善，则停止训练。<br>最后，返回训练过程中的训练损失和验证损失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer</span>(<span class="params">train_loader, valid_loader, model, config, device</span>):</span><br><span class="line"></span><br><span class="line">    criterion = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># Define your loss function, do not modify this.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define your optimization algorithm. </span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config[<span class="string">&#x27;learning_rate&#x27;</span>]*<span class="number">50</span>,</span><br><span class="line">                                 weight_decay=<span class="number">1e-3</span>)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, </span><br><span class="line">                                        T_0=<span class="number">2</span>, T_mult=<span class="number">2</span>, eta_min=config[<span class="string">&#x27;learning_rate&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">&#x27;./models&#x27;</span>):</span><br><span class="line">        os.mkdir(<span class="string">&#x27;./models&#x27;</span>) <span class="comment"># Create directory of saving models.</span></span><br><span class="line"></span><br><span class="line">    n_epochs, best_loss, step, early_stop_count = config[<span class="string">&#x27;n_epochs&#x27;</span>], math.inf, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    train_losses = []</span><br><span class="line">    val_losses = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        model.train() <span class="comment"># Set your model to train mode.</span></span><br><span class="line">        loss_record = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tqdm is a package to visualize your training progress.</span></span><br><span class="line">        train_pbar = tqdm(train_loader, position=<span class="number">0</span>, leave=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_pbar:</span><br><span class="line">            optimizer.zero_grad()               <span class="comment"># Set gradient to zero.</span></span><br><span class="line">            x, y = x.to(device), y.to(device)   <span class="comment"># Move your data to device. </span></span><br><span class="line">            pred = model(x)             </span><br><span class="line">            loss = criterion(pred, y)</span><br><span class="line">            loss.backward()                     <span class="comment"># Compute gradient(backpropagation).</span></span><br><span class="line">            optimizer.step()                    <span class="comment"># Update parameters.</span></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            loss_record.append(loss.detach().item())</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Display current epoch number and loss on tqdm progress bar.</span></span><br><span class="line">            train_pbar.set_description(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;n_epochs&#125;</span>]&#x27;</span>)</span><br><span class="line">            train_pbar.set_postfix(&#123;<span class="string">&#x27;loss&#x27;</span>: loss.detach().item()&#125;)</span><br><span class="line">            </span><br><span class="line">        scheduler.step() </span><br><span class="line"></span><br><span class="line">        mean_train_loss = <span class="built_in">sum</span>(loss_record)/<span class="built_in">len</span>(loss_record)</span><br><span class="line">        train_losses.append(mean_train_loss)</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># Set your model to evaluation mode.</span></span><br><span class="line">        loss_record = []</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                pred = model(x)</span><br><span class="line">                loss = criterion(pred, y)</span><br><span class="line"></span><br><span class="line">            loss_record.append(loss.item())</span><br><span class="line">            </span><br><span class="line">        mean_valid_loss = <span class="built_in">sum</span>(loss_record)/<span class="built_in">len</span>(loss_record)</span><br><span class="line">        val_losses.append(mean_valid_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;n_epochs&#125;</span>]: Train loss: <span class="subst">&#123;mean_train_loss:<span class="number">.4</span>f&#125;</span>, Valid loss: <span class="subst">&#123;mean_valid_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mean_valid_loss &lt; best_loss:</span><br><span class="line">            best_loss = mean_valid_loss</span><br><span class="line">            torch.save(model.state_dict(), config[<span class="string">&#x27;save_path&#x27;</span>]) <span class="comment"># Save your best model</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Saving model with loss &#123;:.3f&#125;...&#x27;</span>.<span class="built_in">format</span>(best_loss))</span><br><span class="line">            early_stop_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            early_stop_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> early_stop_count &gt;= config[<span class="string">&#x27;early_stop&#x27;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\nModel is not improving, so we halt the training session.&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> train_losses, val_losses</span><br></pre></td></tr></table></figure></p>
<h3 id="Configurations"><a href="#Configurations" class="headerlink" title="Configurations"></a>Configurations</h3><p>下面定义了设备（device）和配置参数（config）。<br>根据是否有可用的GPU，将设备设置为’cuda’或’cpu’。<br>参数调整定义了种子，最大循环次数，学习率，早停次数等。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>: <span class="number">10225517</span>,      <span class="comment"># Your seed number, you can pick your lucky number. :)</span></span><br><span class="line">    <span class="string">&#x27;select_all&#x27;</span>: <span class="literal">False</span>,   <span class="comment"># Whether to use all features.</span></span><br><span class="line">    <span class="string">&#x27;valid_ratio&#x27;</span>: <span class="number">0.2</span>,   <span class="comment"># validation_size = train_size * valid_ratio</span></span><br><span class="line">    <span class="string">&#x27;n_epochs&#x27;</span>: <span class="number">5000</span>,     <span class="comment"># Number of epochs.            </span></span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">256</span>, </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">1e-5</span>,              </span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>: <span class="number">500</span>,    <span class="comment"># If model has not improved for this many consecutive epochs, stop training.     </span></span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>: <span class="string">&#x27;./models/model.ckpt&#x27;</span>  <span class="comment"># Your model will be saved here.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><p>这部分代码是用于数据预处理和加载数据的。首先，它设置了一个种子来确保结果的可重现性。然后，它加载了训练数据和测试数据，并将其分割为训练集、验证集和测试集。接下来，它选择了要使用的特征，并对数据进行了归一化处理。最后，它使用PyTorch的DataLoader将数据加载到批次中，以便进行训练和评估模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set seed for reproducibility</span></span><br><span class="line">same_seed(config[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days) </span></span><br><span class="line"><span class="comment"># test_data size: 1078 x 117 (without last day&#x27;s positive rate)</span></span><br><span class="line">train_data, test_data = pd.read_csv(<span class="string">&#x27;./covid.train.csv&#x27;</span>).values, pd.read_csv(<span class="string">&#x27;./covid.test.csv&#x27;</span>).values</span><br><span class="line">train_data, valid_data = train_valid_split(train_data, config[<span class="string">&#x27;valid_ratio&#x27;</span>], config[<span class="string">&#x27;seed&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the data size.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;&quot;&quot;train_data size: <span class="subst">&#123;train_data.shape&#125;</span> </span></span><br><span class="line"><span class="string">valid_data size: <span class="subst">&#123;valid_data.shape&#125;</span> </span></span><br><span class="line"><span class="string">test_data size: <span class="subst">&#123;test_data.shape&#125;</span>&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select features</span></span><br><span class="line">x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#normalization</span></span><br><span class="line">x_min, x_max = x_train.<span class="built_in">min</span>(axis=<span class="number">0</span>), x_train.<span class="built_in">max</span>(axis=<span class="number">0</span>)</span><br><span class="line">x_train = (x_train - x_min) / (x_max - x_min)</span><br><span class="line">x_valid = (x_valid - x_min) / (x_max - x_min)</span><br><span class="line">x_test = (x_test - x_min) / (x_max - x_min)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;&quot;&quot;train_data size: <span class="subst">&#123;x_train.shape&#125;</span> </span></span><br><span class="line"><span class="string">valid_data size: <span class="subst">&#123;x_valid.shape&#125;</span> </span></span><br><span class="line"><span class="string">test_data size: <span class="subst">&#123;x_test.shape&#125;</span>&quot;&quot;&quot;</span>)</span><br><span class="line"><span class="comment"># Print out the number of features.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of features: <span class="subst">&#123;x_train.shape[<span class="number">1</span>]&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train), \</span><br><span class="line">                                            COVID19Dataset(x_valid, y_valid), \</span><br><span class="line">                                            COVID19Dataset(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pytorch data loader loads pytorch dataset into batches.</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">valid_loader = DataLoader(valid_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">False</span>, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Start-training"><a href="#Start-training" class="headerlink" title="Start training"></a>Start training</h3><p>利用之前拟合好的模型进行深度训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = My_Model(input_dim=x_train.shape[<span class="number">1</span>]).to(device) </span><br><span class="line">train_losses, val_losses = trainer(train_loader, valid_loader, model, config, device)</span><br></pre></td></tr></table></figure></p>
<h3 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h3><p>最终保存预测结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_pred</span>(<span class="params">preds, file</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line">model = My_Model(input_dim=x_train.shape[<span class="number">1</span>]).to(device)</span><br><span class="line">model.load_state_dict(torch.load(config[<span class="string">&#x27;save_path&#x27;</span>]))</span><br><span class="line">preds = predict(test_loader, model, device) </span><br><span class="line">save_pred(preds, <span class="string">&#x27;pred.csv&#x27;</span>)      </span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/24/week2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/24/week2/" class="post-title-link" itemprop="url">伯努利模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-24 15:30:42" itemprop="dateCreated datePublished" datetime="2023-07-24T15:30:42+08:00">2023-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-26 15:37:55" itemprop="dateModified" datetime="2023-07-26T15:37:55+08:00">2023-07-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>说明<strong>伯努利模型</strong>的<strong>极大似然估计</strong>以及<strong>贝叶斯估计</strong>中的统计学习方法三要素。<br>伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型n次独立的数据生成结果，其中k次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。</p>
<h3 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h3><p>在统计学中，伯努利模型是一种二元随机变量的概率模型，用于描述只有两种可能结果的随机试验。<br>定义随机变量A为一次伯努利试验的结果，A的取值为[0,1]，概率分布为$P(A)$:</p>
<script type="math/tex; mode=display">P(A=1)=\theta</script><script type="math/tex; mode=display">P(A=0)=1-\theta</script><p>$n$重伯努利试验中$A$发生$k$次记作$ P_{n} (k)=C_{n}^{k}P^{k}(1-P)^{n-k} $，又称二项概率公式。<br>下文统一用$\theta$的值表示结果为1的概率。</p>
<h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><p>伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素分别是<strong>模型</strong>，<strong>策略</strong>和<strong>算法</strong>。<br><strong>模型</strong>：上文提到的伯努利模型，即定义在取值为0与1的随机变量上的概率分布。<br><strong>策略</strong>：极大似然估计和贝叶斯估计的策略都是对数损失函数（贝叶斯估计使用的是结构风险最小化）。<br><strong>算法</strong>：极大似然估计所使用的算法是求取经验风险函数的极小值，贝叶斯估计所使用的算法是求取参数的后验分布，然后计算其期望。</p>
<h3 id="极大似然估计法"><a href="#极大似然估计法" class="headerlink" title="极大似然估计法"></a>极大似然估计法</h3><p>极大似然估计的目标是找到一个参数值$\theta$，使得在给定观察结果的条件下，伯努利模型的似然函数取得最大值。似然函数表示给定参数值下观察到观测结果的概率。<br>对于伯努利模型，似然函数可以表示为：</p>
<script type="math/tex; mode=display">L(\theta)= \prod_{n}^{i=1}P(A_{i})=\theta^{k}(1-\theta)^{n-k}</script><p>为了求解极大似然估计，我们需要最大化似然函数。通常，为了方便计算，我们会对似然函数取对数，得到对数似然函数:</p>
<script type="math/tex; mode=display">\log_{}{L(\theta )} = k\log_{}{\theta } +(n-k)\log_{}{(1-\theta )}</script><p>然后对其求导：</p>
<script type="math/tex; mode=display">\frac{\partial \log_{}{L(\theta )} }{\partial \theta } =\frac{k}{\theta } -\frac{n-k}{1-\theta }</script><p>令$ \frac{\partial \log_{}{L(\theta )} }{\partial \theta } = 0 $，得$\theta =\frac{k}{n}$。<br>记为$ \theta = argmax_{\theta}(L(\theta))$。<br>如果模型为伯努利模型，损失函数是对数损失函数，则求解经验风险最小化等价于最大化似然函数。</p>
<h3 id="贝叶斯估计法"><a href="#贝叶斯估计法" class="headerlink" title="贝叶斯估计法"></a>贝叶斯估计法</h3><p>在贝叶斯估计中，我们将参数$\theta$视为一个随机变量，而不是一个固定但未知的值。我们先引入先验分布，然后，通过贝叶斯定理，我们可以计算在给定观测数据的条件下，参数$\theta$的后验分布。<br>常用的先验分布是Beta分布，因为它是伯努利模型的共轭先验分布。即：</p>
<script type="math/tex; mode=display">f(\theta \mid \alpha ,\beta )=\frac{1}{B(\alpha ,\beta )}\theta ^{\alpha -1}(1-\theta)^{\beta -1}</script><p>那么根据贝叶斯定理，其后验分布为：</p>
<script type="math/tex; mode=display">P(\theta|A_{1},A_{2}...A_{n})=\frac{P(A_{1},A_{2},...A_{n}|\theta)f(\theta)}{P(A_{1},A_{2}, ...A_{n})}\propto P(A_{1},A_{2},...A_{n}|\theta)f(\theta)</script><p>这里正比于的符号，实际上表示后者是前者的核。<br>似然函数</p>
<script type="math/tex; mode=display">P(A_{1},A_{2},...A_{n}|\theta)=\theta^{k}(1-\theta)^{n-k}</script><p>故</p>
<script type="math/tex; mode=display">P(\theta|A_{1},A_{2}...A_{n})\propto \theta^{k}(1-\theta)^{n-k}\cdot \theta ^{\alpha -1}(1-\theta)^{\beta -1}</script><p>即</p>
<script type="math/tex; mode=display">P(\theta|A_{1},A_{2}...A_{n})\propto \theta^{k+\alpha -1}(1-\theta)^{n-k+\beta -1}</script><p>可以看出后验分布其实是Beta分布的核，具体来说是服从参数是$k+\alpha$和$n-k+\beta$的Beta分布。<br>借助Beta分布的期望公式（$E(\theta)=\frac{\alpha }{\alpha +\beta } $），可以得到估计量的值：</p>
<script type="math/tex; mode=display">\theta =\frac{k+\alpha }{k+\alpha +n-k+\beta } =\frac{k+\alpha }{n+\alpha +\beta }</script><p>已知观察结果的条件下使得$\theta$出现概率最大的值。<br>特别的，当$\alpha=\beta=1$时，先验分布服从均匀分布，结果为1的概率$\theta=\frac{k+1}{n+2}\approx \frac{k}{n}$，与最大似然估计法结果一致。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/17/my-first-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="时骅">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="暑期实习博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/17/my-first-blog/" class="post-title-link" itemprop="url">Tensor与Autograd实现线性回归训练</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-17 20:37:18" itemprop="dateCreated datePublished" datetime="2023-07-17T20:37:18+08:00">2023-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-20 19:14:01" itemprop="dateModified" datetime="2023-07-20T19:14:01+08:00">2023-07-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>不使用深度模型框架，只利用<strong>Tensor</strong>和<strong>Autograd</strong>实现一个<strong>线性回归</strong>的训练，要求有输入输出。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>线性回归假设输出与各个输入之间是线性关系：</p>
<script type="math/tex; mode=display">\hat{y} = x_{1} \omega _{1} + x_{2} \omega _{2} + x_{3} \omega _{3} + b</script><p>其中$ \omega $是权重，$ b $是偏差，且均为标量，他们是线性回归模型的参数。<br>模型输出$ \hat{y} $是对真实值$ y $（又称标签）的估计。我们通常允许他们之间有一定的误差。<br>而$ x $则称为样本的特征，用来表征样本的特点。</p>
<p>在机器学习里，将衡量误差的函数称为损失函数。这里我们使用平方误差函数，即：</p>
<script type="math/tex; mode=display">l^{(i)}(\omega _{1},\omega _{2},\omega _{3},b)=\frac{1}{2}(\hat{y} ^{(i)}-y^{(i)} ) ^{2}</script><p>其中常数$ \frac{1}{2} $使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降在深度学习中被广泛使用。<br>线性回归模型的过程中，模型的每个参数将作如下迭代：</p>
<script type="math/tex; mode=display">\omega _{1}\gets\omega _{1}-\frac{\eta }{\left |B \right | }\sum_{i\in B}^{} \frac{\partial l^{(i)}(\omega _{1},\omega _{2},\omega _{3},b)}{\partial \omega _{1}}</script><p>在上式中，$ \left |B  \right | $代表每个小批量中的样本个数，$ \eta $ 称作学习率并取正数。</p>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>设训练数据集样本数为1000，输入个数（特征数）为3。给定随机生成的批量样本特征。<br>使用线性回归模型真实权重$\omega =\left [ 2,-2.3,6 \right ] ^{\top } $和偏差$ b=4.2 $以及一个随机噪声项$ \varepsilon $来生成标签。<br>其中噪声项$ \varepsilon $服从均值为0、标准差为0.01的正态分布。<br>生成数据集代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据集</span></span><br><span class="line">num_inputs = <span class="number">3</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">2.3</span>, <span class="number">6</span>]</span><br><span class="line">true_b = <span class="number">4.5</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs,</span><br><span class="line">                       dtype=torch.float32)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * features[:, <span class="number">2</span>] + true_b</span><br><span class="line">noise = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32)</span><br><span class="line">labels += noise</span><br></pre></td></tr></table></figure></p>
<h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。<br>这里我们定义一个函数：它每次返回batch_size（批量大小）个随机样本的特征和标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历数据集并不断读取小批量数据样本,每次返回batch_size（批量大小）个随机样本的特征和标签。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)]) <span class="comment"># 最后一次可能不足一个batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure></p>
<h3 id="模型函数"><a href="#模型函数" class="headerlink" title="模型函数"></a>模型函数</h3><p>下面是线性回归的矢量计算表达式的实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用mm函数做矩阵乘法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  </span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure><br>用平方损失来定义线性回归的损失函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_pred, y</span>):  </span><br><span class="line">    <span class="keyword">return</span> (y_pred - y.view(y_pred.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><br>以下的sgd函数实现了上一节中介绍的小批量随机梯度下降算法。<br>它通过不断迭代模型参数来优化损失函数。<br>这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。<br>我们将它除以批量大小来得到平均值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>): </span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br></pre></td></tr></table></figure></p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。<br>之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此打开Autograd。<br>学习率设为0.03，迭代周期个数设为3。<br>批量大小最好为2的幂次，设为8。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">0.03</span> <span class="comment">#学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span> <span class="comment">#迭代周期</span></span><br><span class="line">batch_size = <span class="number">8</span> <span class="comment">#批量大小</span></span><br></pre></td></tr></table></figure></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>在训练中，我们将多次迭代模型参数。<br>在每次迭代中，我们根据当前读取的小批量数据样本（特征X和标签y），<br>通过调用反向函数backward计算小批量随机梯度，并调用优化算法sgd迭代模型参数。<br>在一个迭代周期（epoch）中，我们将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):  </span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = squared_loss(linreg(X, w, b), y).<span class="built_in">sum</span>()  <span class="comment"># l是有关小批量X和y的损失之和</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = squared_loss(linreg(features, w, b), labels)</span><br><span class="line">    losses.append(train_l.mean().item())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.012331</span><br><span class="line">epoch 2, loss 0.000054</span><br><span class="line">epoch 3, loss 0.000049</span><br></pre></td></tr></table></figure></p>
<h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p>比较学到的参数和用来生成训练集的真实参数。它们很接近。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure><br>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[2, -2.3, 6]</span><br><span class="line"> tensor([[ 2.0001],</span><br><span class="line">        [-2.3000],</span><br><span class="line">        [ 5.9997]], requires_grad=True)</span><br><span class="line">4.5</span><br><span class="line"> tensor([4.5000], requires_grad=True)</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">时骅</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">时骅</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
